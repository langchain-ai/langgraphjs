{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to wait for user input (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - Implementing [human-in-the-loop](../../concepts/human_in_the_loop) workflows with [interrupt](../../concepts/human_in_the_loop/#interrupt)\n",
    "    - [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional)\n",
    "\n",
    "**Human-in-the-loop (HIL)** interactions are crucial for [agentic systems](../../concepts/agentic_concepts/#human-in-the-loop). Waiting for human input is a common HIL interaction pattern, allowing the agent to ask the user clarifying questions and await input before proceeding. \n",
    "\n",
    "We can implement this in LangGraph using the [interrupt()](/langgraphjs/reference/functions/langgraph.interrupt-1.html) function. `interrupt` allows us to stop graph execution to collect input from a user and continue execution with collected input.\n",
    "\n",
    "This guide demonstrates how to implement human-in-the-loop workflows using LangGraph's [Functional API](../../concepts/functional_api). Specifically, we will demonstrate:\n",
    "\n",
    "1. [A simple usage example](#simple-usage)\n",
    "2. [How to use with a ReAct agent](#agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "!!! note Compatibility\n",
    "\n",
    "    This guide requires `@langchain/langgraph>=0.2.42`.\n",
    "\n",
    "First, install the required dependencies for this example:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use):\n",
    "\n",
    "```typescript\n",
    "process.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n",
    "```\n",
    "\n",
    "!!! tip \"Set up [LangSmith](https://smith.langchain.com) for LangGraph development\"\n",
    "\n",
    "    Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started [here](https://docs.smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple usage\n",
    "\n",
    "Let's demonstrate a simple usage example. We will create three [tasks](../../concepts/functional_api/#task):\n",
    "\n",
    "1. Append `\"bar\"`.\n",
    "2. Pause for human input. When resuming, append human input.\n",
    "3. Append `\"qux\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { task, interrupt } from \"@langchain/langgraph\";\n",
    "\n",
    "const step1 = task(\"step1\", async (inputQuery: string) => {\n",
    "  return `${inputQuery} bar`;\n",
    "});\n",
    "\n",
    "const humanFeedback = task(\n",
    "  \"humanFeedback\",\n",
    "  async (inputQuery: string) => {\n",
    "    const feedback = interrupt(`Please provide feedback: ${inputQuery}`);\n",
    "    return `${inputQuery} ${feedback}`;\n",
    "  });\n",
    "\n",
    "const step3 = task(\"step3\", async (inputQuery: string) => {\n",
    "  return `${inputQuery} qux`;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compose these tasks in a simple [entrypoint](../../concepts/functional_api/#entrypoint):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MemorySaver, entrypoint } from \"@langchain/langgraph\";\n",
    "\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "const graph = entrypoint({\n",
    "  name: \"graph\",\n",
    "  checkpointer,\n",
    "}, async (inputQuery: string) => {\n",
    "  const result1 = await step1(inputQuery);\n",
    "  const result2 = await humanFeedback(result1);\n",
    "  const result3 = await step3(result2);\n",
    "  return result3;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have done to enable human-in-the-loop workflows is call [interrupt()](../../concepts/human_in_the_loop/#interrupt) inside a task.\n",
    "\n",
    "!!! tip\n",
    "\n",
    "    The results of prior tasks - in this case `step1 -- are persisted, so that they are not run again following the `interrupt`.\n",
    "\n",
    "\n",
    "Let's send in a query string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ step1: 'foo bar' }\n",
      "{\n",
      "  __interrupt__: [\n",
      "    {\n",
      "      value: 'Please provide feedback: foo bar',\n",
      "      when: 'during',\n",
      "      resumable: true,\n",
      "      ns: [Array]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = {\n",
    "  configurable: {\n",
    "    thread_id: \"1\"\n",
    "  }\n",
    "};\n",
    "\n",
    "const stream = await graph.stream(\"foo\", config);\n",
    "\n",
    "for await (const event of stream) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've paused with an `interrupt` after `step1`. The interrupt provides instructions to resume the run. To resume, we issue a [Command](../../concepts/human_in_the_loop/#the-command-primitive) containing the data expected by the `humanFeedback` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ humanFeedback: 'foo bar baz' }\n",
      "{ step3: 'foo bar baz qux' }\n",
      "{ graph: 'foo bar baz qux' }\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "const resumeStream = await graph.stream(new Command({\n",
    "  resume: \"baz\"\n",
    "}), config);\n",
    "\n",
    "// Continue execution\n",
    "for await (const event of resumeStream) {\n",
    "  if (event.__metadata__?.cached) {\n",
    "    continue;\n",
    "  }\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resuming, the run proceeds through the remaining step and terminates as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "We will build off of the agent created in the [How to create a ReAct agent using the Functional API](../../how-tos/react-agent-from-scratch-functional) guide.\n",
    "\n",
    "Here we will extend the agent by allowing it to reach out to a human for assistance when needed.\n",
    "\n",
    "### Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. As in the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional), we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://js.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://js.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const model = new ChatOpenAI({\n",
    "  model: \"gpt-4o-mini\",\n",
    "});\n",
    "\n",
    "const getWeather = tool(async ({ location }) => {\n",
    "  // This is a placeholder for the actual implementation\n",
    "  const lowercaseLocation = location.toLowerCase();\n",
    "  if (lowercaseLocation.includes(\"sf\") || lowercaseLocation.includes(\"san francisco\")) {\n",
    "    return \"It's sunny!\";\n",
    "  } else if (lowercaseLocation.includes(\"boston\")) {\n",
    "    return \"It's rainy!\";\n",
    "  } else {\n",
    "    return `I am not sure what the weather is in ${location}`;\n",
    "  }\n",
    "}, {\n",
    "  name: \"getWeather\",\n",
    "  schema: z.object({\n",
    "    location: z.string().describe(\"Location to get the weather for\"),\n",
    "  }),\n",
    "  description: \"Call to get the weather from a specific location.\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach out to a human for assistance, we can simply add a tool that calls [interrupt](../../concepts/human_in_the_loop/#interrupt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { interrupt } from \"@langchain/langgraph\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const humanAssistance = tool(async ({ query }) => {\n",
    "  const humanResponse = interrupt({ query });\n",
    "  return humanResponse.data;\n",
    "}, {\n",
    "  name: \"humanAssistance\",\n",
    "  description: \"Request assistance from a human.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"Human readable question for the human\")\n",
    "  })\n",
    "});\n",
    "\n",
    "const tools = [getWeather, humanAssistance];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "\n",
    "Our tasks are otherwise unchanged from the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional):\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them.\n",
    "\n",
    "We just have one more tool accessible to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  type BaseMessageLike,\n",
    "  AIMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "import { type ToolCall } from \"@langchain/core/messages/tool\";\n",
    "import { task } from \"@langchain/langgraph\";\n",
    "\n",
    "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n",
    "\n",
    "const callModel = task(\"callModel\", async (messages: BaseMessageLike[]) => {\n",
    "  const response = await model.bindTools(tools).invoke(messages);\n",
    "  return response;\n",
    "});\n",
    "\n",
    "const callTool = task(\n",
    "  \"callTool\",\n",
    "  async (toolCall: ToolCall): Promise<AIMessage> => {\n",
    "    const tool = toolsByName[toolCall.name];\n",
    "    const observation = await tool.invoke(toolCall.args);\n",
    "    return new ToolMessage({ content: observation, tool_call_id: toolCall.id });\n",
    "    // Can also pass toolCall directly into the tool to return a ToolMessage\n",
    "    // return tool.invoke(toolCall);\n",
    "  });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define entrypoint\n",
    "\n",
    "Our [entrypoint](../../concepts/functional_api/#entrypoint) is also unchanged from the [ReAct agent guide](../../how-tos/react-agent-from-scratch-functional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { entrypoint, addMessages, MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const agent = entrypoint({\n",
    "  name: \"agent\",\n",
    "  checkpointer: new MemorySaver(),\n",
    "}, async (messages: BaseMessageLike[]) => {\n",
    "  let currentMessages = messages;\n",
    "  let llmResponse = await callModel(currentMessages);\n",
    "  while (true) {\n",
    "    if (!llmResponse.tool_calls?.length) {\n",
    "      break;\n",
    "    }\n",
    "\n",
    "    // Execute tools\n",
    "    const toolResults = await Promise.all(\n",
    "      llmResponse.tool_calls.map((toolCall) => {\n",
    "        return callTool(toolCall);\n",
    "      })\n",
    "    );\n",
    "    \n",
    "    // Append to message list\n",
    "    currentMessages = addMessages(currentMessages, [llmResponse, ...toolResults]);\n",
    "\n",
    "    // Call model again\n",
    "    llmResponse = await callModel(currentMessages);\n",
    "  }\n",
    "\n",
    "  return llmResponse;\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Let's invoke our model with a question that requires human assistance. Our question will also require an invocation of the `getWeather` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrintMessage = (message: BaseMessage) => {\n",
    "  console.log(\"=\".repeat(30), `${message.getType()} message`, \"=\".repeat(30));\n",
    "  console.log(message.content);\n",
    "  if (isAIMessage(message) && message.tool_calls?.length) {\n",
    "    console.log(JSON.stringify(message.tool_calls, null, 2));\n",
    "  }\n",
    "}\n",
    "\n",
    "const prettyPrintStep = (step: Record<string, any>) => {\n",
    "  if (step.__metadata__?.cached) {\n",
    "    return;\n",
    "  }\n",
    "  for (const [taskName, update] of Object.entries(step)) {\n",
    "    const message = update as BaseMessage;\n",
    "    // Only print task updates\n",
    "    if (taskName === \"agent\") continue;\n",
    "    console.log(`\\n${taskName}:`);\n",
    "    if (taskName === \"__interrupt__\") {\n",
    "      console.log(update);\n",
    "    } else {\n",
    "      prettyPrintMessage(message);\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  role: 'user',\n",
      "  content: 'Can you reach out for human assistance: what should I feed my cat? Separately, can you check the weather in San Francisco?'\n",
      "}\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"name\": \"humanAssistance\",\n",
      "    \"args\": {\n",
      "      \"query\": \"What should I feed my cat?\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_TwrNq6tGI61cDCJEpj175h7J\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"getWeather\",\n",
      "    \"args\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    },\n",
      "    \"type\": \"tool_call\",\n",
      "    \"id\": \"call_fMzUBvc0SpZpXxM2LQLXfbke\"\n",
      "  }\n",
      "]\n",
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "It's sunny!\n",
      "\n",
      "__interrupt__:\n",
      "[\n",
      "  {\n",
      "    value: { query: 'What should I feed my cat?' },\n",
      "    when: 'during',\n",
      "    resumable: true,\n",
      "    ns: [ 'callTool:2e0c6c40-9541-57ef-a7af-24213a10d5a4' ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "const userMessage = {\n",
    "  role: \"user\",\n",
    "  content: [\n",
    "    \"Can you reach out for human assistance: what should I feed my cat?\",\n",
    "    \"Separately, can you check the weather in San Francisco?\"\n",
    "  ].join(\" \"),\n",
    "};\n",
    "console.log(userMessage);\n",
    "\n",
    "const agentStream = await agent.stream([userMessage], {\n",
    "  configurable: {\n",
    "    thread_id: \"1\",\n",
    "  }\n",
    "});\n",
    "\n",
    "let lastStep;\n",
    "\n",
    "for await (const step of agentStream) {\n",
    "  prettyPrintStep(step);\n",
    "  lastStep = step;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we generate two tool calls, and although our run is interrupted, we did not block the execution of the `get_weather` tool.\n",
    "\n",
    "Let's inspect where we're interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"__interrupt__\":[{\"value\":{\"query\":\"What should I feed my cat?\"},\"when\":\"during\",\"resumable\":true,\"ns\":[\"callTool:2e0c6c40-9541-57ef-a7af-24213a10d5a4\"]}]}\n"
     ]
    }
   ],
   "source": [
    "console.log(JSON.stringify(lastStep));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can resume execution by issuing a [Command](../../concepts/human_in_the_loop/#the-command-primitive). Note that the data we supply in the `Command` can be customized to your needs based on the implementation of `humanAssistance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "callTool:\n",
      "============================== tool message ==============================\n",
      "You should feed your cat a fish.\n",
      "\n",
      "callModel:\n",
      "============================== ai message ==============================\n",
      "For your cat, it is suggested that you feed it fish. As for the weather in San Francisco, it's currently sunny!\n"
     ]
    }
   ],
   "source": [
    "import { Command } from \"@langchain/langgraph\";\n",
    "\n",
    "const humanResponse = \"You should feed your cat a fish.\";\n",
    "const humanCommand = new Command({\n",
    "  resume: { data: humanResponse },\n",
    "});\n",
    "\n",
    "const resumeStream2 = await agent.stream(humanCommand, config);\n",
    "\n",
    "for await (const step of resumeStream2) {\n",
    "  prettyPrintStep(step);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, when we resume we provide the final tool message, allowing the model to generate its response. Check out the LangSmith traces to see a full breakdown of the runs:\n",
    "\n",
    "1. [Trace from initial query](https://smith.langchain.com/public/c007b042-fdd3-49e7-acbe-cadf6969de4b/r)\n",
    "2. [Trace after resuming](https://smith.langchain.com/public/1cea310a-13f5-4de9-ae1c-045b8b33015e/r)\n",
    "\n",
    "**Note:** The `interrupt` function propagates by throwing a special `GraphInterrupt` error. Therefore, you should avoid using `try/catch` blocks around the `interrupt` function - or if you do, ensure that the `GraphInterrupt` error is thrown again within your `catch` block."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
