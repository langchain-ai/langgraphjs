{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-in-the-loop\n",
    "\n",
    "When creating LangGraph agents, it is often nice to add a human in the loop\n",
    "component. This can be helpful when giving them access to tools. Often in these\n",
    "situations you may want to manually approve an action before taking.\n",
    "\n",
    "This can be in several ways, but the primary supported way is to add an\n",
    "\"interrupt\" before a node is executed. This interrupts execution at that node.\n",
    "You can then resume from that spot to continue.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using either `interruptBefore` or `interruptAfter` in the <code>createReactAgent(model, tools=tool, interruptBefore=[\"tools\" | \"agent\"], interruptAfter=[\"tools\" | \"agent\"])</code> <a href=\"https://langchain-ai.github.io/langgraph/reference/prebuilt/#createreactagent\">API doc</a> constructor. This may be more appropriate if you are used to LangChain's <a href=\"https://js.langchain.com/v0.2/docs/how_to/agent_executor\">AgentExecutor</a> class.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use). Optionally, we\n",
    "can set API key for [LangSmith tracing](https://smith.langchain.com/), which\n",
    "will give us best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"OPENAI_API_KEY\", \"sk_...\");\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"ls__...\");\n",
    "// Deno.env.set(\"LANGCHAIN_CALLBACKS_BACKGROUND\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_PROJECT\", \"Human-in-the-loop: LangGraphJS\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the State\n",
    "\n",
    "The state is the interface for all the nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "interface IState {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => BaseMessage[];\n",
    "    default: () => BaseMessage[];\n",
    "  };\n",
    "}\n",
    "\n",
    "// This defines the agent state\n",
    "const graphState: IState = {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will\n",
    "create a placeholder \"search engine\". However, it is really easy to create your\n",
    "own tools - see the\n",
    "[LangChain documentation](https://js.langchain.com/docs/modules/agents/tools/)\n",
    "on how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.\n",
      "[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,\n",
      "[WARN]: we suggest setting \"process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true\" to avoid additional latency.\n"
     ]
    }
   ],
   "source": [
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchTool = new DynamicStructuredTool({\n",
    "  name: \"search\",\n",
    "  description: \"Call to surf the web.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"The query to use in your search.\"),\n",
    "  }),\n",
    "  func: async ({ query }: { query: string }) => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    // Don't let the LLM know this though ðŸ˜Š\n",
    "    return \"It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\";\n",
    "  },\n",
    "});\n",
    "\n",
    "const tools = [searchTool];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now wrap these tools in a simple\n",
    "[ToolNode](https://langchain-ai.github.io/langgraphjs/reference/classes/prebuilt.ToolNode.html).\n",
    "\n",
    "This is a simple class that takes in a list of messages containing an\n",
    "[AIMessage with tool_calls](https://v02.api.js.langchain.com/classes/langchain_core_messages.AIMessage.html),\n",
    "runs the tools, and returns the output as\n",
    "[ToolMessage](https://v02.api.js.langchain.com/classes/langchain_core_messages_tool.ToolMessage.html)s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const toolNode = new ToolNode(tools);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we need to load the chat model we want to use. Since we are creating a\n",
    "tool-using ReAct agent, we want to make sure the model supports\n",
    "[Tool Calling](https://js.langchain.com/docs/modules/model_io/models/chat/function-calling/)\n",
    "and works with chat messages.\n",
    "\n",
    "Note: these model requirements are not requirements for using LangGraph - they\n",
    "are just requirements for this one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.\n",
      "[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,\n",
      "[WARN]: we suggest setting \"process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true\" to avoid additional latency.\n"
     ]
    }
   ],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({ temperature: 0 });\n",
    "\n",
    "// After we've done this, we should make sure the model knows that it has these tools available to call.\n",
    "// We can do this by binding the tools to the model class.\n",
    "const boundModel = model.bindTools(tools);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the nodes\n",
    "\n",
    "We now need to define a few different nodes in our graph. In `langgraph`, a node\n",
    "can be either a function or a\n",
    "[runnable](https://js.langchain.com/docs/modules/runnables/). There are two main\n",
    "nodes we need for this:\n",
    "\n",
    "1. The agent: responsible for deciding what (if any) actions to take.\n",
    "2. A function to invoke tools: if the agent decides to take an action, this node\n",
    "   will then execute that action.\n",
    "\n",
    "We will also need to define some edges. Some of these edges may be conditional.\n",
    "The reason they are conditional is that based on the output of a node, one of\n",
    "several paths may be taken. The path that is taken is not known until that node\n",
    "is run (the LLM decides).\n",
    "\n",
    "1. Conditional Edge: after the agent is called, we should either: a. If the\n",
    "   agent said to take an action, then the function to invoke tools should be\n",
    "   called\\\n",
    "   b. If the agent said that it was finished, then it should finish\n",
    "2. Normal Edge: after the tools are invoked, it should always go back to the\n",
    "   agent to decide what to do next\n",
    "\n",
    "Let's define the nodes, as well as a function to decide how what conditional\n",
    "edge to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { AIMessage, BaseMessage } from \"@langchain/core/messages\";\n",
    "import { END } from \"@langchain/langgraph\";\n",
    "\n",
    "const routeMessage = (state: { messages: Array<BaseMessage> }) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  // If no tools are called, we can finish (respond to the user)\n",
    "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n",
    "    return END;\n",
    "  }\n",
    "  // Otherwise if there is, we continue and call the tools\n",
    "  return \"tools\";\n",
    "};\n",
    "\n",
    "const callModel = async (\n",
    "  state: { messages: Array<BaseMessage> },\n",
    "  config: RunnableConfig,\n",
    ") => {\n",
    "  const { messages } = state;\n",
    "  const response = await boundModel.invoke(messages, config);\n",
    "  return { messages: [response] };\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We can now put it all together and define the graph!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateGraph {\n",
       "  nodes: {\n",
       "    agent: RunnableLambda {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: { func: \u001b[36m[AsyncFunction: callModel]\u001b[39m },\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[90mundefined\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"runnables\"\u001b[39m ],\n",
       "      func: \u001b[36m[AsyncFunction: callModel]\u001b[39m\n",
       "    },\n",
       "    tools: ToolNode {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: {},\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[32m\"tools\"\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langgraph\"\u001b[39m ],\n",
       "      func: \u001b[36m[Function: func]\u001b[39m,\n",
       "      tags: \u001b[90mundefined\u001b[39m,\n",
       "      config: { tags: [] },\n",
       "      trace: \u001b[33mtrue\u001b[39m,\n",
       "      recurse: \u001b[33mtrue\u001b[39m,\n",
       "      tools: [\n",
       "        DynamicStructuredTool {\n",
       "          lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "          lc_kwargs: \u001b[36m[Object]\u001b[39m,\n",
       "          lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "          name: \u001b[32m\"search\"\u001b[39m,\n",
       "          verbose: \u001b[33mfalse\u001b[39m,\n",
       "          callbacks: \u001b[90mundefined\u001b[39m,\n",
       "          tags: [],\n",
       "          metadata: {},\n",
       "          returnDirect: \u001b[33mfalse\u001b[39m,\n",
       "          description: \u001b[32m\"Call to surf the web.\"\u001b[39m,\n",
       "          func: \u001b[36m[AsyncFunction: func]\u001b[39m,\n",
       "          schema: \u001b[36m[ZodObject]\u001b[39m\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  edges: Set(2) { [ \u001b[32m\"__start__\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ], [ \u001b[32m\"tools\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ] },\n",
       "  branches: {\n",
       "    agent: {\n",
       "      routeMessage: Branch {\n",
       "        condition: \u001b[36m[Function: routeMessage]\u001b[39m,\n",
       "        ends: \u001b[90mundefined\u001b[39m,\n",
       "        then: \u001b[90mundefined\u001b[39m\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  entryPoint: \u001b[90mundefined\u001b[39m,\n",
       "  compiled: \u001b[33mtrue\u001b[39m,\n",
       "  supportMultipleEdges: \u001b[33mtrue\u001b[39m,\n",
       "  channels: {\n",
       "    messages: BinaryOperatorAggregate {\n",
       "      lc_graph_name: \u001b[32m\"BinaryOperatorAggregate\"\u001b[39m,\n",
       "      value: [],\n",
       "      operator: \u001b[36m[Function: value]\u001b[39m,\n",
       "      initialValueFactory: \u001b[36m[Function: default]\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  waitingEdges: Set(0) {}\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "\n",
    "const workflow = new StateGraph({\n",
    "  channels: graphState,\n",
    "});\n",
    "\n",
    "// Define the two nodes we will cycle between\n",
    "workflow.addNode(\"agent\", callModel);\n",
    "workflow.addNode(\"tools\", toolNode);\n",
    "\n",
    "// Set the entrypoint as `agent`\n",
    "// This means that this node is the first one called\n",
    "workflow.addEdge(START, \"agent\");\n",
    "workflow.addConditionalEdges(\"agent\", routeMessage);\n",
    "workflow.addEdge(\"tools\", \"agent\");\n",
    "// **Persistence**\n",
    "// Human-in-the-loop worflows require a checkpointer to ensure\n",
    "// nothing is lost between interactions\n",
    "const checkpointer = new MemorySaver();\n",
    "// **Interrupt**\n",
    "// To always interrupt before a particular node, pass the name of the node to `interruptBefore` when compiling.\n",
    "const graph = workflow.compile({ checkpointer, interruptBefore: [\"tools\"] });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the Agent\n",
    "\n",
    "We can now interact with the agent and see that it stops before calling a tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[human]: hi! I'm bob\n",
      "[ai]: Hello Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrint = (message: BaseMessage) => {\n",
    "  let txt = `[${message._getType()}]: ${message.content}`;\n",
    "  if (\n",
    "    message._getType() === \"ai\" && (message as AIMessage)?.tool_calls?.length ||\n",
    "    0 > 0\n",
    "  ) {\n",
    "    const tool_calls = (message as AIMessage)?.tool_calls?.map(\n",
    "      (tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`,\n",
    "    ).join(\"\\n\");\n",
    "    txt += ` \\nTools: \\n${tool_calls}`;\n",
    "  }\n",
    "  console.log(txt);\n",
    "};\n",
    "\n",
    "const config = { configurable: { thread_id: \"example-thread-1\" } };\n",
    "\n",
    "let inputs = { messages: [new HumanMessage(\"hi! I'm bob\")] };\n",
    "for await (\n",
    "  const { messages } of await graph.stream(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  prettyPrint(messages[messages.length - 1]);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[human]: What did I tell you my name was?\n",
      "[ai]: You mentioned that your name is Bob. How can I help you, Bob?\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [new HumanMessage(\"What did I tell you my name was?\")] };\n",
    "for await (\n",
    "  const { messages } of await graph.stream(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  prettyPrint(messages[messages.length - 1]);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[human]: what's the weather in sf now?\n",
      "[ai]:  \n",
      "Tools: \n",
      "- search({\"query\":\"weather in San Francisco\"})\n"
     ]
    }
   ],
   "source": [
    "inputs = { messages: [new HumanMessage(\"what's the weather in sf now?\")] };\n",
    "for await (\n",
    "  const { messages } of await graph.stream(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  prettyPrint(messages[messages.length - 1]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume**\n",
    "\n",
    "We can now call the agent again with no inputs to continue, ie. run the tool as\n",
    "requested.\n",
    "\n",
    "Running an interrupted graph with `null` as the input means to \"proceed as if\n",
    "the interruption didn't occur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tool]: It's sunny in San Francisco, but you better look out if you're a Gemini ðŸ˜ˆ.\n",
      "[ai]: The current weather in San Francisco is sunny. Enjoy the sunshine! If you have any more questions or need assistance, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "for await (\n",
    "  const { messages } of await graph.stream(null, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })\n",
    ") {\n",
    "  prettyPrint(messages[messages.length - 1]);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
