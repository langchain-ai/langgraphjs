{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add semantic search to your agent's memory\n",
    "\n",
    "This guide shows how to enable semantic search in your agent's memory store. This lets your agent search for items in the long-term memory store by semantic similarity.\n",
    "\n",
    "## Dependencies and environment setup\n",
    "\n",
    "First, install this guide's required dependencies.\n",
    "\n",
    "```bash\n",
    "npm install \\\n",
    "  @langchain/langgraph \\\n",
    "  @langchain/openai \\\n",
    "  @langchain/core \\\n",
    "  uuid \\\n",
    "  zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a memory store with semantic search\n",
    "\n",
    "Here we create our memory store with an [index configuration](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html).\n",
    "\n",
    "By default, stores are configured without semantic/vector search. You can opt in to indexing items when creating the store by providing an [IndexConfig](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html) to the store's constructor.\n",
    "\n",
    "If your store class does not implement this interface, or if you do not pass in an index configuration, semantic search is disabled, and all `index` arguments passed to `put` will have no effect.\n",
    "\n",
    "Now, let's create that store!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings({\n",
    "  model: \"text-embedding-3-small\",\n",
    "});\n",
    "\n",
    "const store = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings,\n",
    "    dims: 1536,\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The anatomy of a memory\n",
    "\n",
    "Before we get into semantic search, let's look at how memories are structured, and how to store them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "let namespace = [\"user_123\", \"memories\"]\n",
    "let memoryKey = \"favorite_food\"\n",
    "let memoryValue = {\"text\": \"I love pizza\"}\n",
    "\n",
    "await store.put(namespace, memoryKey, memoryValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, memories are composed of a namespace, a key, and a value.\n",
    "\n",
    "**Namespaces** are multi-dimensional values (arrays of strings) that allow you to segment memory according to the needs of your application. In this case, we're segmenting memories by user by using a User ID (`\"user_123\"`) as the first dimension of our namespace array.\n",
    "\n",
    "**Keys** are arbitrary strings that identify the memory within the namespace. If you write to the same key in the same namespace multiple times, you'll overwrite the memory that was stored under that key.\n",
    "\n",
    "**Values** are objects that represent the actual memory being stored. These can be any object, so long as its serializable. You can structure these objects according to the needs of your application.\n",
    "\n",
    "## Simple memory retrieval\n",
    "\n",
    "Let's add some more memories to our store and then fetch one of them by it's key to check that it stored properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a tunnel engineer\n"
     ]
    }
   ],
   "source": [
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"italian_food\",\n",
    "  {\"text\": \"I prefer Italian food\"}\n",
    ")\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"spicy_food\",\n",
    "  {\"text\": \"I don't like spicy food\"}\n",
    ")\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"occupation\",\n",
    "  {\"text\": \"I am an airline pilot\"}\n",
    ")\n",
    "\n",
    "// That occupation is too lofty - let's overwrite\n",
    "// it with something more... down-to-earth\n",
    "await store.put(\n",
    "  [\"user_123\", \"memories\"],\n",
    "  \"occupation\",\n",
    "  {\"text\": \"I am a tunnel engineer\"}\n",
    ")\n",
    "\n",
    "// now let's check that our occupation memory was overwritten\n",
    "const occupation = await store.get([\"user_123\", \"memories\"], \"occupation\")\n",
    "console.log(occupation.value.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching memories with natural language\n",
    "\n",
    "Now that we've seen how to store and retrieve memories by namespace and key, let's look at how memories are retrieved using semantic search.\n",
    "\n",
    "Imagine that we had a big pile of memories that we wanted to search, and we didn't know the key that corresponds to the memory that we want to retrieve. Semantic search allows us to search our memory store without keys, by performing a natural language query using text embeddings. We demonstrate this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I am a tunnel engineer (similarity: 0.3070681445327329)\n",
      "Memory: I prefer Italian food (similarity: 0.1435366180543232)\n",
      "Memory: I love pizza (similarity: 0.10650935500808985)\n"
     ]
    }
   ],
   "source": [
    "const memories = await store.search([\"user_123\", \"memories\"], {\n",
    "  query: \"What is my occupation?\",\n",
    "  limit: 3,\n",
    "});\n",
    "\n",
    "for (const memory of memories) {\n",
    "  console.log(`Memory: ${memory.value.text} (similarity: ${memory.score})`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example: Long-term semantic memory in a ReAct agent\n",
    "\n",
    "Let's look at a simple example of providing an agent with long-term memory.\n",
    "\n",
    "Long-term memory can be thought of in two phases: storage, and recall.\n",
    "\n",
    "In the example below we handle storage by giving the agent a tool that it can use to create new memories.\n",
    "\n",
    "To handle recall we'll add a prompt step that queries the memory store using the text from the user's chat message. We'll then inject the results of that query into the system message.\n",
    "\n",
    "### Simple memory storage tool\n",
    "\n",
    "Let's start off by creating a tool that lets the LLM store new memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const upsertMemoryTool = tool(async (\n",
    "  { content },\n",
    "  config: LangGraphRunnableConfig\n",
    "): Promise<string> => {\n",
    "  const store = config.store as InMemoryStore;\n",
    "  if (!store) {\n",
    "    throw new Error(\"No store provided to tool.\");\n",
    "  }\n",
    "  await store.put(\n",
    "    [\"user_123\", \"memories\"],\n",
    "    uuidv4(), // give each memory its own unique ID\n",
    "    { text: content }\n",
    "  );\n",
    "  return \"Stored memory.\";\n",
    "}, {\n",
    "  name: \"upsert_memory\",\n",
    "  schema: z.object({\n",
    "    content: z.string().describe(\"The content of the memory to store.\"),\n",
    "  }),\n",
    "  description: \"Upsert long-term memories.\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tool above, we use a UUID as the key so that the memory store can accumulate memories endlessly without worrying about key conflicts. We do this instead of accumulating memories into a single object or array because the memory store indexes items by key. Giving each memory its own key in the store allows each memory to be assigned its own unique embedding vector that can be matched to the search query.\n",
    "\n",
    "### Simple semantic recall mechanism\n",
    "\n",
    "Now that we have a tool for storing memories, let's create a prompt function that we can use with `createReactAgent` to handle the recall mechanism.\n",
    "\n",
    "Note that if we weren't using `createReactAgent` here, you could use this same function as the first node in your graph and it would work just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesAnnotation } from \"@langchain/langgraph\";\n",
    "\n",
    "const addMemories = async (\n",
    "  state: typeof MessagesAnnotation.State,\n",
    "  config: LangGraphRunnableConfig\n",
    ") => {\n",
    "  const store = config.store as InMemoryStore;\n",
    "\n",
    "  if (!store) {\n",
    "    throw new Error(\"No store provided to state modifier.\");\n",
    "  }\n",
    "  \n",
    "  // Search based on user's last message\n",
    "  const items = await store.search(\n",
    "    [\"user_123\", \"memories\"], \n",
    "    { \n",
    "      // Assume it's not a complex message\n",
    "      query: state.messages[state.messages.length - 1].content as string,\n",
    "      limit: 4 \n",
    "    }\n",
    "  );\n",
    "\n",
    "  \n",
    "  const memories = items.length \n",
    "    ? `## Memories of user\\n${\n",
    "      items.map(item => `${item.value.text} (similarity: ${item.score})`).join(\"\\n\")\n",
    "    }`\n",
    "    : \"\";\n",
    "\n",
    "  // Add retrieved memories to system message\n",
    "  return [\n",
    "    { role: \"system\", content: `You are a helpful assistant.\\n${memories}` },\n",
    "    ...state.messages\n",
    "  ];\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Finally, let's put it all together into an agent, using `createReactAgent`. Notice that we're not adding a checkpointer here. The examples below will not be reusing message history. All details not contained in the input messages will be coming from the recall mechanism defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n",
    "  tools: [upsertMemoryTool],\n",
    "  prompt: addMemories,\n",
    "  store: store\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our sample agent\n",
    "\n",
    "Now that we've got everything put together, let's test it out!\n",
    "\n",
    "First, let's define a helper function that we can use to print messages in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {\n",
    "  BaseMessage,\n",
    "  isSystemMessage,\n",
    "  isAIMessage,\n",
    "  isHumanMessage,\n",
    "  isToolMessage,\n",
    "  AIMessage,\n",
    "  HumanMessage,\n",
    "  ToolMessage,\n",
    "  SystemMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "\n",
    "function printMessages(messages: BaseMessage[]) {\n",
    "  for (const message of messages) {\n",
    "    if (isSystemMessage(message)) {\n",
    "      const systemMessage = message as SystemMessage;\n",
    "      console.log(`System: ${systemMessage.content}`);\n",
    "    } else if (isHumanMessage(message)) {\n",
    "      const humanMessage = message as HumanMessage;\n",
    "      console.log(`User: ${humanMessage.content}`);\n",
    "    } else if (isAIMessage(message)) {\n",
    "      const aiMessage = message as AIMessage;\n",
    "      if (aiMessage.content) {\n",
    "        console.log(`Assistant: ${aiMessage.content}`);\n",
    "      }\n",
    "      if (aiMessage.tool_calls) {\n",
    "        for (const toolCall of aiMessage.tool_calls) {\n",
    "          console.log(`\\t${toolCall.name}(${JSON.stringify(toolCall.args)})`);\n",
    "        }\n",
    "      }\n",
    "    } else if (isToolMessage(message)) {\n",
    "      const toolMessage = message as ToolMessage;\n",
    "      console.log(\n",
    "        `\\t\\t${toolMessage.name} -> ${JSON.stringify(toolMessage.content)}`\n",
    "      );\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we run the agent and print the message, we can see that the agent remembers the food preferences that we added to the store at the very beginning of this demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I'm hungry. What should I eat?\n",
      "Assistant: Since you prefer Italian food and love pizza, how about ordering a pizza? You could choose a classic Margherita or customize it with your favorite toppings, making sure to keep it non-spicy. Enjoy your meal!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "let result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"I'm hungry. What should I eat?\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing new memories\n",
    "\n",
    "Now that we know that the recall mechanism works, let's see if we can get our example agent to store a new memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Please remember that every Thursday is trash day.\n",
      "\tupsert_memory({\"content\":\"Every Thursday is trash day.\"})\n",
      "\t\tupsert_memory -> \"Stored memory.\"\n",
      "Assistant: I've remembered that every Thursday is trash day!\n"
     ]
    }
   ],
   "source": [
    "result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"Please remember that every Thursday is trash day.\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that it has stored it, let's see if it remembers.\n",
    "\n",
    "Remember - there's no checkpointer here. Every time we invoke the agent it's an entirely new conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: When am I supposed to take out the garbage?\n",
      "Assistant: You take out the garbage every Thursday, as it's trash day for you.\n"
     ]
    }
   ],
   "source": [
    "result = await agent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"When am I supposed to take out the garbage?\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "\n",
    "printMessages(result.messages);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "The example above is quite simple, but hopefully it helps you to imagine how you might interweave storage and recall mechanisms into your agents. In the sections below we touch on a few more topics that might help you as you get into more advanced use cases.\n",
    "\n",
    "### Multi-vector indexing\n",
    "\n",
    "You can store and search different aspects of memories separately to improve recall or to omit certain fields from the semantic indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem 2\n",
      "Item: mem2; Score(0.58961641225287)\n",
      "Memory: Ate alone at home\n",
      "Emotion: felt a bit lonely\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "// Configure store to embed both memory content and emotional context\n",
    "const multiVectorStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    fields: [\"memory\", \"emotional_context\"],\n",
    "  },\n",
    "});\n",
    "\n",
    "// Store memories with different content/emotion pairs\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"Had pizza with friends at Mario's\",\n",
    "  emotional_context: \"felt happy and connected\",\n",
    "  this_isnt_indexed: \"I prefer ravioli though\",\n",
    "});\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"Ate alone at home\",\n",
    "  emotional_context: \"felt a bit lonely\",\n",
    "  this_isnt_indexed: \"I like pie\",\n",
    "});\n",
    "\n",
    "// Search focusing on emotional state - matches mem2\n",
    "const results = await multiVectorStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"times they felt isolated\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "console.log(\"Expect mem 2\");\n",
    "\n",
    "for (const r of results) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "  console.log(`Emotion: ${r.value.emotional_context}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override fields at storage time\n",
    "You can override which fields to embed when storing a specific memory using `put(..., { index: [...fields] })`, regardless of the store's default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem1\n",
      "Item: mem1; Score(0.3375009515587189)\n",
      "Memory: I love spicy food\n",
      "Expect mem2\n",
      "Item: mem2; Score(0.1920732213417712)\n",
      "Memory: I love spicy food\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const overrideStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    // Default to embed memory field\n",
    "    fields: [\"memory\"],\n",
    "  }\n",
    "});\n",
    "\n",
    "// Store one memory with default indexing\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "});\n",
    "\n",
    "// Store another memory, overriding which fields to embed\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "  // Override: only embed the context\n",
    "  index: [\"context\"]\n",
    "});\n",
    "\n",
    "// Search about food - matches mem1 (using default field)\n",
    "console.log(\"Expect mem1\");\n",
    "const results2 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"what food do they like\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results2) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}\n",
    "\n",
    "// Search about restaurant atmosphere - matches mem2 (using overridden field)\n",
    "console.log(\"Expect mem2\");\n",
    "const results3 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"restaurant environment\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results3) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
