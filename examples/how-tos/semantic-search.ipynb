{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add semantic search to your agent's memory\n",
    "\n",
    "This guide shows how to enable semantic search in your agent's memory store. This lets search for items in the store by semantic similarity.\n",
    "\n",
    "First, install this guide's required dependencies.\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/core uuid zod\n",
    "```\n",
    "\n",
    "Next, we need to set API keys for OpenAI (the LLM we will use)\n",
    "\n",
    "```bash\n",
    "export OPENAI_API_KEY=your-api-key\n",
    "```\n",
    "\n",
    "Optionally, we can set API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_CALLBACKS_BACKGROUND=\"true\"\n",
    "export LANGCHAIN_API_KEY=your-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the store with an [index configuration](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html). By default, stores are configured without semantic/vector search. You can opt in to indexing items when creating the store by providing an [IndexConfig](https://langchain-ai.github.io/langgraphjs/reference/interfaces/checkpoint.IndexConfig.html) to the store's constructor. If your store class does not implement this interface, or if you do not pass in an index configuration, semantic search is disabled, and all `index` arguments passed to `put` will have no effect. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAIEmbeddings } from \"@langchain/openai\";\n",
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const embeddings = new OpenAIEmbeddings({\n",
    "  model: \"text-embedding-3-small\",\n",
    "});\n",
    "\n",
    "const store = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings,\n",
    "    dims: 1536,\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's store some memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Store some memories\n",
    "await store.put([\"user_123\", \"memories\"], \"1\", {\"text\": \"I love pizza\"})\n",
    "await store.put([\"user_123\", \"memories\"], \"2\", {\"text\": \"I prefer Italian food\"})\n",
    "await store.put([\"user_123\", \"memories\"], \"3\", {\"text\": \"I don't like spicy food\"})\n",
    "await store.put([\"user_123\", \"memories\"], \"3\", {\"text\": \"I am studying econometrics\"})\n",
    "await store.put([\"user_123\", \"memories\"], \"3\", {\"text\": \"I am a plumber\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search memories using natural language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I prefer Italian food (similarity: 0.4657744498860293)\n",
      "Memory: I love pizza (similarity: 0.3743831559964955)\n",
      "Memory: I am a plumber (similarity: 0.17918150007138176)\n"
     ]
    }
   ],
   "source": [
    "// Find memories about food preferences\n",
    "\n",
    "const memories = await store.search([\"user_123\", \"memories\"], {\n",
    "  query: \"I like food?\",\n",
    "  limit: 5,\n",
    "});\n",
    "\n",
    "for (const memory of memories) {\n",
    "  console.log(`Memory: ${memory.value.text} (similarity: ${memory.score})`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in your agent\n",
    "\n",
    "Add semantic search to any node by injecting the store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "import { MessagesAnnotation, LangGraphRunnableConfig } from \"@langchain/langgraph\";\n",
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { getContextVariable } from \"@langchain/core/context\";\n",
    "\n",
    "import { z } from \"zod\";\n",
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "\n",
    "const addMemories = async (state: typeof MessagesAnnotation.State, config: LangGraphRunnableConfig) => {\n",
    "  const store = config.store;\n",
    "  // Search based on user's last message\n",
    "  const items = await store.search(\n",
    "    [\"user_123\", \"memories\"], \n",
    "    { \n",
    "      // Assume it's not a complex message\n",
    "      query: state.messages[state.messages.length - 1].content as string,\n",
    "      limit: 2 \n",
    "    }\n",
    "  );\n",
    "  \n",
    "  const memories = items.length \n",
    "    ? `## Memories of user\\n${items.map(item => item.value.text).join(\"\\n\")}`\n",
    "    : \"\";\n",
    "\n",
    "  // Add retrieved memories to system message\n",
    "  return [\n",
    "    { role: \"system\", content: `You are a helpful assistant.\\n${memories}` },\n",
    "    ...state.messages\n",
    "  ];\n",
    "};\n",
    "\n",
    "const upsertMemoryTool = tool(async (\n",
    "  input,\n",
    "  config: LangGraphRunnableConfig\n",
    "): Promise<string> => {\n",
    "  const store = config.store;\n",
    "  if (!store) {\n",
    "    throw new Error(\"No store provided to tool.\");\n",
    "  }\n",
    "  const memoryId = getContextVariable(\"memoryId\") || uuidv4();\n",
    "  await store.put(\n",
    "    [\"user_123\", \"memories\"],\n",
    "    memoryId,\n",
    "    { text: input.content }\n",
    "  );\n",
    "  return `Stored memory ${memoryId}`;\n",
    "}, {\n",
    "  name: \"upsert_memory\",\n",
    "  schema: z.object({\n",
    "    content: z.string(),\n",
    "  }),\n",
    "  description: \"Upsert a memory in the database.\",\n",
    "});\n",
    "\n",
    "const agent = createReactAgent({\n",
    "  llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n",
    "  tools: [upsertMemoryTool],\n",
    "  stateModifier: addMemories,\n",
    "  store: store,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the agent, we can see that it remembers that we added a memory about liking Italian food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you in the mood for? Maybe some Italian food or pizza?\n"
     ]
    }
   ],
   "source": [
    "const stream = await agent.stream({\n",
    "  messages: [{\n",
    "    role: \"user\",\n",
    "    content: \"I'm hungry\",\n",
    "  }],\n",
    "}, {\n",
    "  streamMode: \"messages\",\n",
    "});\n",
    "\n",
    "for await (const [message, _metadata] of stream) {\n",
    "  console.log(message.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Usage\n",
    "\n",
    "### Multi-vector indexing\n",
    "\n",
    "Store and search different aspects of memories separately to improve recall or omit certain fields from being indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem 2\n",
      "Item: mem2; Score(0.5895009051069847)\n",
      "Memory: Ate alone at home\n",
      "Emotion: felt a bit lonely\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "// Configure store to embed both memory content and emotional context\n",
    "const multiVectorStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    fields: [\"memory\", \"emotional_context\"],\n",
    "  },\n",
    "});\n",
    "\n",
    "// Store memories with different content/emotion pairs\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"Had pizza with friends at Mario's\",\n",
    "  emotional_context: \"felt happy and connected\",\n",
    "  this_isnt_indexed: \"I prefer ravioli though\",\n",
    "});\n",
    "await multiVectorStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"Ate alone at home\",\n",
    "  emotional_context: \"felt a bit lonely\",\n",
    "  this_isnt_indexed: \"I like pie\",\n",
    "});\n",
    "\n",
    "// Search focusing on emotional state - matches mem2\n",
    "const results = await multiVectorStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"times they felt isolated\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "console.log(\"Expect mem 2\");\n",
    "\n",
    "for (const r of results) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "  console.log(`Emotion: ${r.value.emotional_context}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override fields at storage time\n",
    "You can override which fields to embed when storing a specific memory using `put(..., { index: [...fields] })`, regardless of the store's default configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect mem1\n",
      "Item: mem1; Score(0.337496867367478)\n",
      "Memory: I love spicy food\n",
      "Expect mem2\n",
      "Item: mem1; Score(0.1921202784760764)\n",
      "Memory: I love spicy food\n"
     ]
    }
   ],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const overrideStore = new InMemoryStore({\n",
    "  index: {\n",
    "    embeddings: embeddings,\n",
    "    dims: 1536,\n",
    "    // Default to embed memory field\n",
    "    fields: [\"memory\"],\n",
    "  }\n",
    "});\n",
    "\n",
    "// Store one memory with default indexing\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem1\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "});\n",
    "\n",
    "// Store another overriding which fields to embed\n",
    "await overrideStore.put([\"user_123\", \"memories\"], \"mem2\", {\n",
    "  memory: \"I love spicy food\",\n",
    "  context: \"At a Thai restaurant\",\n",
    "  // Override: only embed the context\n",
    "  index: [\"context\"]\n",
    "});\n",
    "\n",
    "// Search about food - matches mem1 (using default field)\n",
    "console.log(\"Expect mem1\");\n",
    "const results2 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"what food do they like\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results2) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}\n",
    "\n",
    "// Search about restaurant atmosphere - matches mem2 (using overridden field)\n",
    "console.log(\"Expect mem2\");\n",
    "const results3 = await overrideStore.search([\"user_123\", \"memories\"], {\n",
    "  query: \"restaurant environment\",\n",
    "  limit: 1,\n",
    "});\n",
    "\n",
    "for (const r of results3) {\n",
    "  console.log(`Item: ${r.key}; Score(${r.score})`);\n",
    "  console.log(`Memory: ${r.value.memory}`);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
