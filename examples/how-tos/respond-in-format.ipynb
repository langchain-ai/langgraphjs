{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499df080",
   "metadata": {},
   "source": [
    "# Respond in a format\n",
    "\n",
    "The typical ReAct agent prompts the LLM to respond in 1 of two formats: a\n",
    "function call (~ JSON) to use a tool, or conversational text to respond to the\n",
    "user.\n",
    "\n",
    "If your agent is connected to a structured (or even generative) UI, or if it is\n",
    "communicating with another agent or software process, you may want it to resopnd\n",
    "in a specific structured format.\n",
    "\n",
    "In this example we will build a conversational ReAct agent that responds in a\n",
    "specific format. We will do this by using\n",
    "[tool calling](https://js.langchain.com/docs/modules/model_io/models/chat/function-calling/).\n",
    "This is useful when you want to enforce that an agent's response is in a\n",
    "specific format. In this example, we will ask it respond as if it were a\n",
    "weatherman, returning the temperature and additional info in separate,\n",
    "machine-readable fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de475e2b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the packages required\n",
    "\n",
    "```bash\n",
    "yarn add langchain @langchain/anthropic @langchain/langgraph\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2eb83",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for Anthropic (the LLM we will use).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26368af",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"ANTHROPIC_API_KEY\", \"sk_...\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e2855",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for\n",
    "[LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"ls...\");\n",
    "Deno.env.set(\"LANGCHAIN_CALLBACKS_BACKGROUND\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_PROJECT\", \"Respond in Format: LangGraphJS\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629d5cd",
   "metadata": {},
   "source": [
    "## Set up the State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ab504e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "const graphState = {\n",
    "  messages: {\n",
    "    value: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d68c2",
   "metadata": {},
   "source": [
    "## Set up the tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac780faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchTool = new DynamicStructuredTool({\n",
    "  name: \"search\",\n",
    "  description: \"Call to surf the web.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"The query to use in your search.\"),\n",
    "  }),\n",
    "  func: async ({ query }: { query: string }) => {\n",
    "    // This is a placeholder, but don't tell the LLM that...\n",
    "    return \"The answer to your question lies within.\";\n",
    "  },\n",
    "});\n",
    "\n",
    "const tools = [searchTool];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f2978",
   "metadata": {},
   "source": [
    "We can now wrap these tools in a simple\n",
    "[ToolNode](https://langchain-ai.github.io/langgraphjs/reference/classes/prebuilt.ToolNode.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b22819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const toolNode = new ToolNode(tools);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb369c",
   "metadata": {},
   "source": [
    "## Set up the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db695bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "\n",
    "const model = new ChatAnthropic({\n",
    "  temperature: 0,\n",
    "  model: \"claude-3-haiku-20240307\",\n",
    "});\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074af33",
   "metadata": {},
   "source": [
    "After we've done this, we should make sure the model knows that it has these\n",
    "tools available to call. We can do this by converting the LangChain tools into\n",
    "the format for function calling, and then bind them to the model class.\n",
    "\n",
    "We also want to define a response schema for the language model and bind it to\n",
    "the model as a function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c394188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "\n",
    "const Response = z.object({\n",
    "  temperature: z.number().describe(\"the temperature\"),\n",
    "  other_notes: z.string().describe(\"any other notes about the weather\"),\n",
    "});\n",
    "\n",
    "const boundModel = model.bindTools([\n",
    "  ...tools,\n",
    "  { name: \"Response\", schema: Response },\n",
    "]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afa5a4",
   "metadata": {},
   "source": [
    "## Define the nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7a65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import { AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "const route = (state: { messages: BaseMessage[] }) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1] as AIMessage;\n",
    "  // If there is no function call, then we finish\n",
    "  if (!lastMessage.tool_calls || lastMessage.tool_calls.length === 0) {\n",
    "    return \"end\";\n",
    "  }\n",
    "  // Otherwise if there is, we need to check what type of function call it is\n",
    "  if (lastMessage.tool_calls[0].name === \"Response\") {\n",
    "    return \"end\";\n",
    "  }\n",
    "  // Otherwise we continue\n",
    "  return \"action\";\n",
    "};\n",
    "\n",
    "// Define the function that calls the model\n",
    "const callModel = async (state: { messages: BaseMessage[] }) => {\n",
    "  const { messages } = state;\n",
    "  const response = await boundModel.invoke(messages);\n",
    "  // We return an object, because this will get added to the existing list\n",
    "  return { messages: [response] };\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee60938",
   "metadata": {},
   "source": [
    "## Define the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d4584bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateGraph {\n",
       "  nodes: {\n",
       "    agent: RunnableLambda {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: { func: \u001b[36m[AsyncFunction: callModel]\u001b[39m },\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[90mundefined\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"runnables\"\u001b[39m ],\n",
       "      func: \u001b[36m[AsyncFunction: callModel]\u001b[39m\n",
       "    },\n",
       "    action: ToolNode {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: {},\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[32m\"tools\"\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langgraph\"\u001b[39m ],\n",
       "      func: \u001b[36m[Function: func]\u001b[39m,\n",
       "      tags: \u001b[90mundefined\u001b[39m,\n",
       "      config: { tags: [] },\n",
       "      trace: \u001b[33mtrue\u001b[39m,\n",
       "      recurse: \u001b[33mtrue\u001b[39m,\n",
       "      tools: [\n",
       "        DynamicStructuredTool {\n",
       "          lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "          lc_kwargs: \u001b[36m[Object]\u001b[39m,\n",
       "          lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "          name: \u001b[32m\"search\"\u001b[39m,\n",
       "          verbose: \u001b[33mfalse\u001b[39m,\n",
       "          callbacks: \u001b[90mundefined\u001b[39m,\n",
       "          tags: [],\n",
       "          metadata: {},\n",
       "          returnDirect: \u001b[33mfalse\u001b[39m,\n",
       "          description: \u001b[32m\"Call to surf the web.\"\u001b[39m,\n",
       "          func: \u001b[36m[AsyncFunction: func]\u001b[39m,\n",
       "          schema: \u001b[36m[ZodObject]\u001b[39m\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  edges: Set(2) { [ \u001b[32m\"__start__\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ], [ \u001b[32m\"action\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ] },\n",
       "  branches: {\n",
       "    agent: {\n",
       "      route: Branch {\n",
       "        condition: \u001b[36m[Function: route]\u001b[39m,\n",
       "        ends: { action: \u001b[32m\"action\"\u001b[39m, end: \u001b[32m\"__end__\"\u001b[39m },\n",
       "        then: \u001b[90mundefined\u001b[39m\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  entryPoint: \u001b[90mundefined\u001b[39m,\n",
       "  compiled: \u001b[33mtrue\u001b[39m,\n",
       "  supportMultipleEdges: \u001b[33mtrue\u001b[39m,\n",
       "  channels: {\n",
       "    messages: BinaryOperatorAggregate {\n",
       "      lc_graph_name: \u001b[32m\"BinaryOperatorAggregate\"\u001b[39m,\n",
       "      value: [],\n",
       "      operator: \u001b[36m[Function: value]\u001b[39m,\n",
       "      initialValueFactory: \u001b[36m[Function: default]\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  waitingEdges: Set(0) {}\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { END, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph({\n",
    "  channels: graphState,\n",
    "});\n",
    "\n",
    "// Define the two nodes we will cycle between\n",
    "workflow.addNode(\"agent\", callModel);\n",
    "workflow.addNode(\"action\", toolNode);\n",
    "\n",
    "// Set the entrypoint as `agent`\n",
    "// This means that this node is the first one called\n",
    "workflow.setEntryPoint(\"agent\");\n",
    "\n",
    "// We now add a conditional edge\n",
    "workflow.addConditionalEdges(\n",
    "  // First, we define the start node. We use `agent`.\n",
    "  // This means these are the edges taken after the `agent` node is called.\n",
    "  \"agent\",\n",
    "  // Next, we pass in the function that will determine which node is called next.\n",
    "  route,\n",
    "  {\n",
    "    action: \"action\",\n",
    "    end: END,\n",
    "  },\n",
    ");\n",
    "\n",
    "// We now add a normal edge from `tools` to `agent`.\n",
    "// This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.addEdge(\"action\", \"agent\");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661243c5",
   "metadata": {},
   "source": [
    "## Use it!\n",
    "\n",
    "We can now use it! This now exposes the\n",
    "[same interface](https://v02.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html) as all\n",
    "other LangChain runnables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb4d5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[human]: what is the weather in sf\n",
      "\n",
      "---\n",
      "\n",
      "[ai]: [object Object] \n",
      "Tools: \n",
      "- search({\"query\":\"weather in sf\"})\n",
      "\n",
      "---\n",
      "\n",
      "[tool]: The answer to your question lies within.\n",
      "\n",
      "---\n",
      "\n",
      "[ai]: [object Object],[object Object] \n",
      "Tools: \n",
      "- Response({\"temperature\":65,\"other_notes\":\"Sunny with a high of 70°F and a low of 55°F. Light winds around 10 mph.\"})\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const prettyPrint = (message: BaseMessage) => {\n",
    "  let txt = `[${message._getType()}]: ${message.content}`;\n",
    "  if (\n",
    "    (message._getType() === \"ai\" &&\n",
    "      (message as AIMessage)?.tool_calls?.length) ||\n",
    "    0 > 0\n",
    "  ) {\n",
    "    const tool_calls = (message as AIMessage)?.tool_calls\n",
    "      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n",
    "      .join(\"\\n\");\n",
    "    txt += ` \\nTools: \\n${tool_calls}`;\n",
    "  }\n",
    "  console.log(txt);\n",
    "};\n",
    "\n",
    "const inputs = {\n",
    "  messages: [new HumanMessage(\"what is the weather in sf\")],\n",
    "};\n",
    "\n",
    "for await (const output of await app.stream(inputs, { streamMode: \"values\" })) {\n",
    "  const { messages } = output;\n",
    "  prettyPrint(messages[messages.length - 1]);\n",
    "  console.log(\"\\n---\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416e63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
