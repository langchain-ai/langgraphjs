{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2eecb96-cf0e-47ed-8116-88a7eaa4236d",
   "metadata": {},
   "source": [
    "# How to add cross-thread persistence to your graph\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Prerequisites</p>\n",
    "    <p>\n",
    "        This guide assumes familiarity with the following:\n",
    "        <ul>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/persistence/\">\n",
    "                    Persistence\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://langchain-ai.github.io/langgraphjs/concepts/memory/\">\n",
    "                    Memory\n",
    "                </a>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"https://js.langchain.com/docs/concepts/#chat-models\">\n",
    "                    Chat Models\n",
    "                </a>\n",
    "            </li>             \n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "In the [previous guide](https://langchain-ai.github.io/langgraphjs/how-tos/persistence.ipynb) you learned how to persist graph state across multiple interactions on a single [thread](). LangGraph.js also allows you to persist data across **multiple threads**. For instance, you can store information about users (their names or preferences) in a shared memory and reuse them in the new conversational threads.\n",
    "\n",
    "In this guide, we will show how to construct and use a graph that has a shared memory implemented using the [Store](https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html) interface.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "    Support for the <code><a href=\"https://langchain-ai.github.io/langgraphjs/reference/classes/checkpoint.BaseStore.html\">Store</a></code> API that is used in this guide was added in LangGraph.js <code>v0.2.10</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// process.env.LANGCHAIN_API_KEY = \"lsv2__...\";\n",
    "// process.env.ANTHROPIC_API_KEY = \"your api key\";\n",
    "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "// process.env.LANGCHAIN_PROJECT = \"Cross-thread persistence: LangGraphJS\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c550b5-1954-496b-8b9d-800361af17dc",
   "metadata": {},
   "source": [
    "## Define store\n",
    "\n",
    "In this example we will create a graph that will be able to retrieve information about a user's preferences. We will do so by defining an `InMemoryStore` - an object that can store data in memory and query that data. We will then pass the store object when compiling the graph. This allows each node in the graph to access the store: when you define node functions, you can define `store` keyword argument, and LangGraph will automatically pass the store object you compiled the graph with.\n",
    "\n",
    "When storing objects using the `Store` interface you define two things:\n",
    "\n",
    "* the namespace for the object, a tuple (similar to directories)\n",
    "* the object key (similar to filenames)\n",
    "\n",
    "In our example, we'll be using `(\"memories\", <userId>)` as namespace and random UUID as key for each new memory.\n",
    "\n",
    "Importantly, to determine the user, we will be passing `userId` via the config keyword argument of the node function.\n",
    "\n",
    "Let's first define an `InMemoryStore` which is already populated with some memories about the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f303d6-612e-4e34-bf36-29d4ed25d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { InMemoryStore } from \"@langchain/langgraph\";\n",
    "\n",
    "const inMemoryStore = new InMemoryStore();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3389c9f4-226d-40c7-8bfc-ee8aac24f79d",
   "metadata": {},
   "source": [
    "## Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30a362-528c-45ee-9df6-630d2d843588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { v4 as uuidv4 } from \"uuid\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "import {\n",
    "  Annotation,\n",
    "  StateGraph,\n",
    "  START,\n",
    "  MemorySaver,\n",
    "  LangGraphRunnableConfig,\n",
    "  messagesStateReducer,\n",
    "} from \"@langchain/langgraph\";\n",
    "\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  messages: Annotation<BaseMessage[]>({\n",
    "    reducer: messagesStateReducer,\n",
    "    default: () => [],\n",
    "  }),\n",
    "});\n",
    "\n",
    "const model = new ChatAnthropic({ modelName: \"claude-3-5-sonnet-20240620\" });\n",
    "\n",
    "// NOTE: we're passing the Store param to the node --\n",
    "// this is the Store we compile the graph with\n",
    "const callModel = async (\n",
    "  state: typeof StateAnnotation.State,\n",
    "  config: LangGraphRunnableConfig\n",
    "): Promise<{ messages: any }> => {\n",
    "  const store = config.store;\n",
    "  if (!store) {\n",
    "    if (!store) {\n",
    "      throw new Error(\"store is required when compiling the graph\");\n",
    "    }\n",
    "  }\n",
    "  if (!config.configurable?.userId) {\n",
    "    throw new Error(\"userId is required in the config\");\n",
    "  }\n",
    "  const namespace = [\"memories\", config.configurable?.userId];\n",
    "  const memories = await store.search(namespace);\n",
    "  const info = memories.map((d) => d.value.data).join(\"\\n\");\n",
    "  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\n",
    "\n",
    "  // Store new memories if the user asks the model to remember\n",
    "  const lastMessage = state.messages[state.messages.length - 1];\n",
    "  if (\n",
    "    typeof lastMessage.content === \"string\" &&\n",
    "    lastMessage.content.toLowerCase().includes(\"remember\")\n",
    "  ) {\n",
    "    await store.put(namespace, uuidv4(), { data: lastMessage.content });\n",
    "  }\n",
    "\n",
    "  const response = await model.invoke([\n",
    "    { type: \"system\", content: systemMsg },\n",
    "    ...state.messages,\n",
    "  ]);\n",
    "  return { messages: response };\n",
    "};\n",
    "\n",
    "const builder = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"call_model\", callModel)\n",
    "  .addEdge(START, \"call_model\");\n",
    "\n",
    "// NOTE: we're passing the store object here when compiling the graph\n",
    "const graph = builder.compile({\n",
    "  checkpointer: new MemorySaver(),\n",
    "  store: inMemoryStore,\n",
    "});\n",
    "// If you're using LangGraph Cloud or LangGraph Studio, you don't need to pass the store or checkpointer when compiling the graph, since it's done automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a4a18-67e4-4f0b-b655-a29bbe202e1c",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        If you're using LangGraph Cloud or LangGraph Studio, you <strong>don't need</strong> to pass store when compiling the graph, since it's done automatically.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d4e33-556d-4fa5-8094-2a076bc21529",
   "metadata": {},
   "source": [
    "## Run the graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842c626-6cd9-4f58-b549-58978e478098",
   "metadata": {},
   "source": [
    "Now let's specify a user ID in the config and tell the model our name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c871a073-a466-46ad-aafe-2b870831057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"ef28a40a-fd75-4478-929a-5413f2a6b044\",\n",
      "  \"content\": \"Hi! Remember: my name is Bob\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "  \"content\": \"Hello Bob! It's nice to meet you. I'll remember that your name is Bob. How can I assist you today?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"output_tokens\": 29\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01UcHJnSAuVDFuDmqaYkxWAf\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 28,\n",
      "      \"output_tokens\": 29\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 28,\n",
      "    \"output_tokens\": 29,\n",
      "    \"total_tokens\": 57\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "let config = { configurable: { thread_id: \"1\", userId: \"1\" } };\n",
    "let inputMessage = { type: \"user\", content: \"Hi! Remember: my name is Bob\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d862be40-1f8a-4057-81c4-b7bf073dc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"eaaa4e1c-1560-4b0a-9c2d-396313cb000c\",\n",
      "  \"content\": \"what is my name?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "  \"content\": \"Your name is Bob. It's nice to meet you, Bob!\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 33,\n",
      "      \"output_tokens\": 17\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01VfqUerYCND1JuWGvbnAacP\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 33,\n",
      "      \"output_tokens\": 17\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 33,\n",
      "    \"output_tokens\": 17,\n",
      "    \"total_tokens\": 50\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = { configurable: { thread_id: \"2\", userId: \"1\" } };\n",
    "inputMessage = { type: \"user\", content: \"what is my name?\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd01ec-f135-4811-8743-daff8daea422",
   "metadata": {},
   "source": [
    "We can now inspect our in-memory store and verify that we have in fact saved the memories for the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76cde493-89cf-4709-a339-207d2b7e9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ data: 'Hi! Remember: my name is Bob' }\n"
     ]
    }
   ],
   "source": [
    "const memories = await inMemoryStore.search([\"memories\", \"1\"]);\n",
    "for (const memory of memories) {\n",
    "    console.log(await memory.value);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5d7eb-af23-4131-b8fd-2a69e74e6e55",
   "metadata": {},
   "source": [
    "Let's now run the graph for another user to verify that the memories about the first user are self contained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d362350b-d730-48bd-9652-983812fd7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage {\n",
      "  \"id\": \"1006b149-de8d-4d8e-81f4-c78c51a7144b\",\n",
      "  \"content\": \"what is my name?\",\n",
      "  \"additional_kwargs\": {},\n",
      "  \"response_metadata\": {}\n",
      "}\n",
      "AIMessage {\n",
      "  \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "  \"content\": \"I apologize, but I don't have any information about your name or personal details. As an AI assistant, I don't have access to personal information about individual users unless it's specifically provided in our conversation. Is there something else I can help you with?\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"output_tokens\": 56\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"id\": \"msg_01MjpYZ65NjwZMYq42BWa2Ze\",\n",
      "    \"model\": \"claude-3-5-sonnet-20240620\",\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "      \"input_tokens\": 25,\n",
      "      \"output_tokens\": 56\n",
      "    },\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\"\n",
      "  },\n",
      "  \"tool_calls\": [],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 25,\n",
      "    \"output_tokens\": 56,\n",
      "    \"total_tokens\": 81\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "config = { configurable: { thread_id: \"3\", userId: \"2\" } };\n",
    "inputMessage = { type: \"user\", content: \"what is my name?\" };\n",
    "\n",
    "for await (const chunk of await graph.stream(\n",
    "  { messages: [inputMessage] },\n",
    "  { ...config, streamMode: \"values\" }\n",
    ")) {\n",
    "  console.log(chunk.messages[chunk.messages.length - 1]);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
