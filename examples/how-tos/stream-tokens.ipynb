{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9395cccb-a6d0-4d46-bad7-ed2d012af58a",
   "metadata": {},
   "source": [
    "# Streaming Tokens\n",
    "\n",
    "In this example, we will stream tokens from the language model powering an agent. We will use a ReAct agent as an example. The tl;dr is to use [streamEvents](https://js.langchain.com/v0.2/docs/how_to/chat_streaming/#stream-events) ([API Ref](https://v01.api.js.langchain.com/classes/langchain_core_runnables.Runnable.html#streamEvents)) and make sure you `stream()` the model within your node.\n",
    "\n",
    "This how-to guide closely follows the others in this directory, showing how to incorporate the functionality into a prototypical agent in LangGraph.\n",
    "\n",
    "This works for [StateGraph](https://langchain-ai.github.io/langgraphjs/reference/classes/index.StateGraph.html) and all its subclasses, such as [MessageGraph](https://langchain-ai.github.io/langgraphjs/reference/classes/index.MessageGraph.html).\n",
    "\n",
    "<div class=\"admonition info\">\n",
    "    <p class=\"admonition-title\">Streaming Support</p>\n",
    "    <p>\n",
    "        Token streaming is supported by many, but not all chat models. Check to see if your LLM integration supports token streaming <a href=\"https://js.langchain.com/v0.2/docs/integrations/chat/\">here (doc)</a>. Note that some integrations may support _general_ token streaming but lack support for streaming tool calls.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the <code>createReactAgent(model, tools=tool)</code> (<a href=\"https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html\">API doc</a>) constructor. This may be more appropriate if you are used to LangChain’s <a href=\"https://python.langchain.com/v0.1/docs/modules/agents/concepts/#agentexecutor\">AgentExecutor</a> class.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "## Setup\n",
    "\n",
    "This guide will use OpenAI's GPT-4o model. We will optionally set our API key for [LangSmith tracing](https://smith.langchain.com/), which will give us best-in-class observability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05706ae-f5c9-45e8-8c0a-2215703ee993",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"OPENAI_API_KEY\", \"sk_...\");\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"ls__...\");\n",
    "Deno.env.set(\"LANGCHAIN_CALLBACKS_BACKGROUND\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_PROJECT\", \"Time Travel: LangGraphJS\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47c4dc-cabe-4ee5-aaa8-9552f5d75ed2",
   "metadata": {},
   "source": [
    "## Define the state\n",
    "\n",
    "The state is the interface for all of the nodes in our graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abea6e1f-c21c-4dfe-a4cd-89326e625c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "interface IState {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => BaseMessage[];\n",
    "    default: () => BaseMessage[];\n",
    "  };\n",
    "  next: string;\n",
    "}\n",
    "\n",
    "// This defines the agent state\n",
    "const State: IState = {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408be71e-8d06-403a-8623-6e6c222c677a",
   "metadata": {},
   "source": [
    "## Set up the tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will\n",
    "use create a placeholder search engine. However, it is really easy to create\n",
    "your own tools - see documentation\n",
    "[here](https://js.langchain.com/v0.2/docs/how_to/custom_tools) on how to do\n",
    "that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae6c71e-10ce-4783-9dfc-49e5b750b269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ \u001b[32m\"Cold, with a low of 3℃\"\u001b[39m ]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { DynamicStructuredTool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchTool = new DynamicStructuredTool({\n",
    "  name: \"search\",\n",
    "  description:\n",
    "    \"Use to surf the web, fetch current information, check the weather, and retrieve other information.\",\n",
    "  schema: z.object({\n",
    "    query: z.string().describe(\"The query to use in your search.\"),\n",
    "  }),\n",
    "  func: async ({ query }: { query: string }) => {\n",
    "    // This is a placeholder for the actual implementation\n",
    "    return [\"Cold, with a low of 3℃\"];\n",
    "  },\n",
    "});\n",
    "\n",
    "await searchTool.invoke({ query: \"What's the weather like?\" });\n",
    "\n",
    "const tools = [searchTool];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eae3ed-b322-4afe-9111-d7dc204fdb77",
   "metadata": {},
   "source": [
    "We can now wrap these tools in a simple\n",
    "[ToolNode](https://langchain-ai.github.io/langgraphjs/reference/classes/prebuilt.ToolNode.html).\n",
    "This object will actually run the tools (functions) whenever they are invoked by\n",
    "our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638fa6e9-cb76-4838-bc8b-02edf7da5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const toolNode = new ToolNode(tools);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94b49b-fb11-463c-b34c-9d66b5327678",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "\n",
    "Now we will load the\n",
    "[chat model](https://js.langchain.com/v0.2/docs/concepts/#chat-models).\n",
    "\n",
    "1. It should work with messages. We will represent all agent state in the form\n",
    "   of messages, so it needs to be able to work well with them.\n",
    "2. It should work with\n",
    "   [tool calling](https://js.langchain.com/v0.2/docs/how_to/tool_calling/#passing-tools-to-llms),\n",
    "   meaning it can return function arguments in its response.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        These model requirements are not general requirements for using LangGraph - they are just requirements for this one example.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a00435f-5e80-4a3e-a873-b308b1f781db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({ model: \"gpt-4o\" });\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975c1ab-fcac-4448-a0cd-7969ae6f4e45",
   "metadata": {},
   "source": [
    "After we've done this, we should make sure the model knows that it has these\n",
    "tools available to call. We can do this by calling\n",
    "[bindTools](https://v01.api.js.langchain.com/classes/langchain_core_language_models_chat_models.BaseChatModel.html#bindTools).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0d4a57f-e7d0-48d7-8626-e83185c1236d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  tools: [\n",
       "    {\n",
       "      type: \u001b[32m\"function\"\u001b[39m,\n",
       "      function: {\n",
       "        name: \u001b[32m\"search\"\u001b[39m,\n",
       "        description: \u001b[32m\"Use to surf the web, fetch current information, check the weather, and retrieve other information.\"\u001b[39m,\n",
       "        parameters: {\n",
       "          type: \u001b[32m\"object\"\u001b[39m,\n",
       "          properties: \u001b[36m[Object]\u001b[39m,\n",
       "          required: \u001b[36m[Array]\u001b[39m,\n",
       "          additionalProperties: \u001b[33mfalse\u001b[39m,\n",
       "          \u001b[32m\"$schema\"\u001b[39m: \u001b[32m\"http://json-schema.org/draft-07/schema#\"\u001b[39m\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const boundModel = model.bindTools(tools);\n",
    "boundModel.kwargs;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0626d5-faae-4e24-bf1b-f01357c28627",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We can now put it all together. Time travel requires a checkpointer to save the\n",
    "state - otherwise you wouldn't have anything go `get` or `update`. We will use\n",
    "the\n",
    "[MemorySaver](https://langchain-ai.github.io/langgraphjs/reference/classes/index.MemorySaver.html),\n",
    "which \"saves\" checkpoints in-memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "812b6d4b-9db7-490b-adae-ad9d933da56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateGraph {\n",
       "  nodes: {\n",
       "    agent: RunnableLambda {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: { func: \u001b[36m[AsyncFunction: callModel]\u001b[39m },\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[90mundefined\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"runnables\"\u001b[39m ],\n",
       "      func: \u001b[36m[AsyncFunction: callModel]\u001b[39m\n",
       "    },\n",
       "    tools: ToolNode {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: {},\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[32m\"tools\"\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langgraph\"\u001b[39m ],\n",
       "      func: \u001b[36m[Function: func]\u001b[39m,\n",
       "      tags: \u001b[90mundefined\u001b[39m,\n",
       "      config: { tags: [] },\n",
       "      trace: \u001b[33mtrue\u001b[39m,\n",
       "      recurse: \u001b[33mtrue\u001b[39m,\n",
       "      tools: [\n",
       "        DynamicStructuredTool {\n",
       "          lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "          lc_kwargs: \u001b[36m[Object]\u001b[39m,\n",
       "          lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "          name: \u001b[32m\"search\"\u001b[39m,\n",
       "          verbose: \u001b[33mfalse\u001b[39m,\n",
       "          callbacks: \u001b[90mundefined\u001b[39m,\n",
       "          tags: [],\n",
       "          metadata: {},\n",
       "          returnDirect: \u001b[33mfalse\u001b[39m,\n",
       "          description: \u001b[32m\"Use to surf the web, fetch current information, check the weather, and retrieve other information.\"\u001b[39m,\n",
       "          func: \u001b[36m[AsyncFunction: func]\u001b[39m,\n",
       "          schema: \u001b[36m[ZodObject]\u001b[39m\n",
       "        }\n",
       "      ]\n",
       "    }\n",
       "  },\n",
       "  edges: Set(2) { [ \u001b[32m\"__start__\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ], [ \u001b[32m\"tools\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ] },\n",
       "  branches: {\n",
       "    agent: {\n",
       "      routeMessage: Branch {\n",
       "        condition: \u001b[36m[Function: routeMessage]\u001b[39m,\n",
       "        ends: \u001b[90mundefined\u001b[39m,\n",
       "        then: \u001b[90mundefined\u001b[39m\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  entryPoint: \u001b[90mundefined\u001b[39m,\n",
       "  compiled: \u001b[33mtrue\u001b[39m,\n",
       "  supportMultipleEdges: \u001b[33mtrue\u001b[39m,\n",
       "  channels: {\n",
       "    messages: BinaryOperatorAggregate {\n",
       "      lc_graph_name: \u001b[32m\"BinaryOperatorAggregate\"\u001b[39m,\n",
       "      value: [],\n",
       "      operator: \u001b[36m[Function: value]\u001b[39m,\n",
       "      initialValueFactory: \u001b[36m[Function: default]\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  waitingEdges: Set(0) {}\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { END, START, StateGraph } from \"@langchain/langgraph\";\n",
    "import { AIMessageChunk, BaseMessage } from \"@langchain/core/messages\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "\n",
    "const routeMessage = (state: { messages: Array<BaseMessage> }) => {\n",
    "  const { messages } = state;\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "  // If no tools are called, we can finish (respond to the user)\n",
    "  if (!lastMessage?.tool_calls?.length) {\n",
    "    return END;\n",
    "  }\n",
    "  // Otherwise if there is, we continue and call the tools\n",
    "  return \"tools\";\n",
    "};\n",
    "\n",
    "const callModel = async (\n",
    "  state: { messages: Array<BaseMessage> },\n",
    "  config: RunnableConfig,\n",
    ") => {\n",
    "  const { messages } = state;\n",
    "  const streamOut = await boundModel.stream(messages, config);\n",
    "  let finalMessage: AIMessageChunk | null = null;\n",
    "  for await (const chunk of streamOut) {\n",
    "    if (finalMessage === null) {\n",
    "      finalMessage = chunk;\n",
    "    } else {\n",
    "      finalMessage = finalMessage.concat(chunk);\n",
    "    }\n",
    "  }\n",
    "  return { messages: [finalMessage] };\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph({\n",
    "  channels: State,\n",
    "});\n",
    "\n",
    "// Define the two nodes we will cycle between\n",
    "workflow.addNode(\"agent\", callModel);\n",
    "workflow.addNode(\"tools\", toolNode);\n",
    "\n",
    "// Set the entrypoint as `agent`\n",
    "workflow.addEdge(START, \"agent\");\n",
    "workflow.addConditionalEdges(\"agent\", routeMessage);\n",
    "workflow.addEdge(\"tools\", \"agent\");\n",
    "\n",
    "const graph = workflow.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d8e1d",
   "metadata": {},
   "source": [
    "## Call streamEvents\n",
    "\n",
    "We can now interact with the agent. Between interactions you can get and update\n",
    "state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24711ca1-3fd8-46ca-b2c1-affa8cd45267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello\n",
      " Jo\n",
      "!\n",
      " How\n",
      " can\n",
      " I\n",
      " assist\n",
      " you\n",
      " today\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import { ChatGenerationChunk } from \"@langchain/core/outputs\";\n",
    "import { AIMessageChunk } from \"@langchain/core/messages\";\n",
    "let config = { configurable: { thread_id: \"conversation-num-1\" } };\n",
    "let inputs = { messages: [[\"user\", \"Hi I'm Jo.\"]] };\n",
    "for await (\n",
    "  const event of await graph.streamEvents(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "    version: \"v1\",\n",
    "  })\n",
    ") {\n",
    "  if (event.event === \"on_llm_stream\") {\n",
    "    let chunk: ChatGenerationChunk = event.data?.chunk;\n",
    "    let msg = chunk.message as AIMessageChunk;\n",
    "    if (msg.tool_call_chunks && msg.tool_call_chunks.length > 0) {\n",
    "      console.log(msg.tool_call_chunks);\n",
    "    } else {\n",
    "      console.log(msg.content);\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c68d3",
   "metadata": {},
   "source": [
    "## How to stream tool calls\n",
    "\n",
    "Many providers support token-level streaming of tool invocations. To get the\n",
    "partially populated results, you can access the message chunks'\n",
    "`tool_call_chunks` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "634a8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    name: \"search\",\n",
      "    args: \"\",\n",
      "    id: \"call_P0UsjSBXstqMbMZpSTWEzxqL\",\n",
      "    index: 0\n",
      "  }\n",
      "]\n",
      "[ { name: undefined, args: '{\"', id: undefined, index: 0 } ]\n",
      "[ { name: undefined, args: \"query\", id: undefined, index: 0 } ]\n",
      "[ { name: undefined, args: '\":\"', id: undefined, index: 0 } ]\n",
      "[ { name: undefined, args: \"current\", id: undefined, index: 0 } ]\n",
      "[ { name: undefined, args: \" weather\", id: undefined, index: 0 } ]\n",
      "[ { name: undefined, args: '\"}', id: undefined, index: 0 } ]\n",
      "\n",
      "\n",
      "The\n",
      " weather\n",
      " today\n",
      " is\n",
      " cold\n",
      ",\n",
      " with\n",
      " a\n",
      " low\n",
      " temperature\n",
      " of\n",
      " \n",
      "3\n",
      "℃\n",
      ".\n",
      " Stay\n",
      " warm\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for await (\n",
    "  const event of await graph.streamEvents(\n",
    "    { messages: [[\"user\", \"What's the weather like today?\"]] },\n",
    "    {\n",
    "      ...config,\n",
    "      streamMode: \"values\",\n",
    "      version: \"v1\",\n",
    "    },\n",
    "  )\n",
    ") {\n",
    "  if (event.event === \"on_llm_stream\") {\n",
    "    let chunk: ChatGenerationChunk = event.data?.chunk;\n",
    "    let msg = chunk.message as AIMessageChunk;\n",
    "    if (msg.tool_call_chunks && msg.tool_call_chunks.length > 0) {\n",
    "      console.log(msg.tool_call_chunks);\n",
    "    } else {\n",
    "      console.log(msg.content);\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43897a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
