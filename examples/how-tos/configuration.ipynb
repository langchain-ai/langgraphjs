{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Once you've created an app in LangGraph, you likely will want to permit configuration at runtime.\n",
    "\n",
    "For instance, you may want to let the LLM or prompt be selected dynamically, configure a user's `user_id` to enforce row-level security, etc.\n",
    "\n",
    "In LangGraph, configuration and other [\"out-of-band\" communication](https://en.wikipedia.org/wiki/Out-of-band) is done via the [RunnableConfig](https://v02.api.js.langchain.com/interfaces/langchain_core_runnables.RunnableConfig.html), which is always the second positional arg when invoking your application.\n",
    "\n",
    "Below, we walk through an example of letting you configure a user ID and pick which model to use.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This guide will use Anthropic's Claude 3 Haiku and OpenAI's GPT-4o model. We will optionally set our API key\n",
    "for [LangSmith tracing](https://smith.langchain.com/), which will give us\n",
    "best-in-class observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"OPENAI_API_KEY\", \"sk_...\");\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"ls__...\");\n",
    "// Deno.env.set(\"LANGCHAIN_CALLBACKS_BACKGROUND\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_PROJECT\", \"Configuration: LangGraphJS\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the graph\n",
    "\n",
    "We will create an exceedingly simple message graph for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.\n",
      "[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,\n",
      "[WARN]: we suggest setting \"process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true\" to avoid additional latency.\n",
      "[WARN]: You have enabled LangSmith tracing without backgrounding callbacks.\n",
      "[WARN]: If you are not using a serverless environment where you must wait for tracing calls to finish,\n",
      "[WARN]: we suggest setting \"process.env.LANGCHAIN_CALLBACKS_BACKGROUND=true\" to avoid additional latency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StateGraph {\n",
       "  nodes: {\n",
       "    fetchUserInfo: RunnableLambda {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: { func: \u001b[36m[AsyncFunction: fetchUserInformation]\u001b[39m },\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[90mundefined\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"runnables\"\u001b[39m ],\n",
       "      func: \u001b[36m[AsyncFunction: fetchUserInformation]\u001b[39m\n",
       "    },\n",
       "    agent: RunnableLambda {\n",
       "      lc_serializable: \u001b[33mfalse\u001b[39m,\n",
       "      lc_kwargs: { func: \u001b[36m[AsyncFunction: callModel]\u001b[39m },\n",
       "      lc_runnable: \u001b[33mtrue\u001b[39m,\n",
       "      name: \u001b[90mundefined\u001b[39m,\n",
       "      lc_namespace: [ \u001b[32m\"langchain_core\"\u001b[39m, \u001b[32m\"runnables\"\u001b[39m ],\n",
       "      func: \u001b[36m[AsyncFunction: callModel]\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  edges: Set(3) {\n",
       "    [ \u001b[32m\"__start__\"\u001b[39m, \u001b[32m\"fetchUserInfo\"\u001b[39m ],\n",
       "    [ \u001b[32m\"fetchUserInfo\"\u001b[39m, \u001b[32m\"agent\"\u001b[39m ],\n",
       "    [ \u001b[32m\"agent\"\u001b[39m, \u001b[32m\"__end__\"\u001b[39m ]\n",
       "  },\n",
       "  branches: {},\n",
       "  entryPoint: \u001b[90mundefined\u001b[39m,\n",
       "  compiled: \u001b[33mtrue\u001b[39m,\n",
       "  supportMultipleEdges: \u001b[33mtrue\u001b[39m,\n",
       "  channels: {\n",
       "    messages: BinaryOperatorAggregate {\n",
       "      lc_graph_name: \u001b[32m\"BinaryOperatorAggregate\"\u001b[39m,\n",
       "      value: [],\n",
       "      operator: \u001b[36m[Function: value]\u001b[39m,\n",
       "      initialValueFactory: \u001b[36m[Function: default]\u001b[39m\n",
       "    },\n",
       "    userInfo: BinaryOperatorAggregate {\n",
       "      lc_graph_name: \u001b[32m\"BinaryOperatorAggregate\"\u001b[39m,\n",
       "      value: \u001b[32m\"N/A\"\u001b[39m,\n",
       "      operator: \u001b[36m[Function: value]\u001b[39m,\n",
       "      initialValueFactory: \u001b[36m[Function: default]\u001b[39m\n",
       "    }\n",
       "  },\n",
       "  waitingEdges: Set(0) {}\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatAnthropic } from \"@langchain/anthropic\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { RunnableConfig } from \"@langchain/core/runnables\";\n",
    "import { END, MemorySaver, START, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "interface IState {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => BaseMessage[];\n",
    "    default: () => BaseMessage[];\n",
    "  };\n",
    "  userInfo: {\n",
    "    value: (x: string, y: string) => string;\n",
    "    default: () => string;\n",
    "  };\n",
    "}\n",
    "\n",
    "// This defines the agent state\n",
    "const graphState: IState = {\n",
    "  messages: {\n",
    "    value: (x: BaseMessage[], y: BaseMessage[]) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "  userInfo: {\n",
    "    value: (x?: string, y?: string) => {\n",
    "      return y ? y : x ? x : \"N/A\";\n",
    "    },\n",
    "    default: () => \"N/A\",\n",
    "  },\n",
    "};\n",
    "\n",
    "const promptTemplate = ChatPromptTemplate.fromMessages([\n",
    "  [\"system\", \"You are a helpful assistant.\\n\\n## User Info:\\n{userInfo}\"],\n",
    "  [\"placeholder\", \"{messages}\"],\n",
    "]);\n",
    "\n",
    "const callModel = async (\n",
    "  state: { messages: Array<BaseMessage>; userInfo: string },\n",
    "  config: RunnableConfig\n",
    ") => {\n",
    "  const { messages, userInfo } = state;\n",
    "  const modelName = config?.configurable?.model;\n",
    "  const model =\n",
    "    modelName === \"claude\"\n",
    "      ? new ChatAnthropic({ model: \"claude-3-haiku-20240307\" })\n",
    "      : new ChatOpenAI({ model: \"gpt-4o\" });\n",
    "  const chain = promptTemplate.pipe(model);\n",
    "  const response = await chain.invoke(\n",
    "    {\n",
    "      messages,\n",
    "      userInfo,\n",
    "    },\n",
    "    config\n",
    "  );\n",
    "  return { messages: [response] };\n",
    "};\n",
    "\n",
    "const workflow = new StateGraph({\n",
    "  channels: graphState,\n",
    "});\n",
    "\n",
    "const fetchUserInformation = async (\n",
    "  _: { messages: Array<BaseMessage> },\n",
    "  config: RunnableConfig\n",
    ") => {\n",
    "  const userDB = {\n",
    "    user1: {\n",
    "      name: \"John Doe\",\n",
    "      email: \"jod@langchain.ai\",\n",
    "      phone: \"+1234567890\",\n",
    "    },\n",
    "    user2: {\n",
    "      name: \"Jane Doe\",\n",
    "      email: \"jad@langchain.ai\",\n",
    "      phone: \"+0987654321\",\n",
    "    },\n",
    "  };\n",
    "  const userId = config?.configurable?.user;\n",
    "  if (userId) {\n",
    "    const user = userDB[userId];\n",
    "    if (user) {\n",
    "      return {\n",
    "        userInfo: `Name: ${user.name}\\nEmail: ${user.email}\\nPhone: ${user.phone}`,\n",
    "      };\n",
    "    }\n",
    "  }\n",
    "  return { userInfo: \"N/A\" };\n",
    "};\n",
    "\n",
    "// Define the two nodes we will cycle between\n",
    "workflow.addNode(\"fetchUserInfo\", fetchUserInformation);\n",
    "workflow.addNode(\"agent\", callModel);\n",
    "\n",
    "// Set the entrypoint as `fetchUserInfo`\n",
    "// so we can always start from there\n",
    "workflow.addEdge(START, \"fetchUserInfo\");\n",
    "workflow.addEdge(\"fetchUserInfo\", \"agent\");\n",
    "workflow.addEdge(\"agent\", END);\n",
    "\n",
    "// Here we only save in-memory\n",
    "let memory = new MemorySaver();\n",
    "const graph = workflow.compile({ checkpointer: memory });\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call with config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Could you remind me of my email??\n",
      "-----\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, John! Your email is jod@langchain.ai. How can I assist you further?\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let config = {\n",
    "  configurable: {\n",
    "    model: \"openai\",\n",
    "    user: \"user1\",\n",
    "  },\n",
    "};\n",
    "let inputs = { messages: [new HumanMessage(\"Could you remind me of my email??\")] };\n",
    "for await (const { messages } of await graph.stream(inputs, {\n",
    "  ...config,\n",
    "  streamMode: \"values\",\n",
    "})) {\n",
    "  let msg = messages[messages?.length - 1];\n",
    "  if (msg?.content) {\n",
    "    console.log(msg.content);\n",
    "  } else if (msg?.tool_calls?.length > 0) {\n",
    "    console.log(msg.tool_calls);\n",
    "  } else {\n",
    "    console.log(msg);\n",
    "  }\n",
    "  console.log(\"-----\\n\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the config\n",
    "\n",
    "Now let's try the same input with a different user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Could you remind me of my email??\n",
      "-----\n",
      "\n",
      "Of course, Jane! Your email is jad@langchain.ai. If you need any further assistance, feel free to ask!\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "let config = {\n",
    "    configurable: {\n",
    "      model: \"openai\",\n",
    "      user: \"user2\",\n",
    "    },\n",
    "  };\n",
    "  let inputs = { messages: [new HumanMessage(\"Could you remind me of my email??\")] };\n",
    "  for await (const { messages } of await graph.stream(inputs, {\n",
    "    ...config,\n",
    "    streamMode: \"values\",\n",
    "  })) {\n",
    "    let msg = messages[messages?.length - 1];\n",
    "    if (msg?.content) {\n",
    "      console.log(msg.content);\n",
    "    } else if (msg?.tool_calls?.length > 0) {\n",
    "      console.log(msg.tool_calls);\n",
    "    } else {\n",
    "      console.log(msg);\n",
    "    }\n",
    "    console.log(\"-----\\n\");\n",
    "  }\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [LangSmith Trace (link)](https://smith.langchain.com/public/bbd3561f-c0d1-4886-ae18-a6626c6b8670/r/946098b5-84d3-4456-a03c-5dbc8591e76b) for this run to \"see what the LLM sees\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
