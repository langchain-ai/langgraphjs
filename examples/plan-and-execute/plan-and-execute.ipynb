{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan-and-Execute\n",
    "\n",
    "This notebook shows how to create a \"plan-and-execute\" style agent. This is\n",
    "heavily inspired by the [Plan-and-Solve](https://arxiv.org/abs/2305.04091) paper\n",
    "as well as the [Baby-AGI](https://github.com/yoheinakajima/babyagi) project.\n",
    "\n",
    "The core idea is to first come up with a multi-step plan, and then go through\n",
    "that plan one item at a time. After accomplishing a particular task, you can\n",
    "then revisit the plan and modify as appropriate.\n",
    "\n",
    "This compares to a typical [ReAct](https://arxiv.org/abs/2210.03629) style agent\n",
    "where you think one step at a time. The advantages of this \"plan-and-execute\"\n",
    "style agent are:\n",
    "\n",
    "1. Explicit long term planning (which even really strong LLMs can struggle with)\n",
    "2. Ability to use smaller/weaker models for the execution step, only using\n",
    "   larger/better models for the planning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to install the packages required.\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for OpenAI (the LLM we will use) and Tavily (the\n",
    "search tool we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"OPENAI_API_KEY\", \"YOUR_API_KEY\")\n",
    "// Deno.env.set(\"TAVILY_API_KEY\", \"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can set API key for LangSmith tracing, which will give us\n",
    "best-in-class observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\")\n",
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"YOUR_API_KEY\")\n",
    "// Deno.env.set(\"LANGCHAIN_PROJECT\", \"YOUR_PROJECT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {} }"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We will first define the tools we want to use. For this simple example, we will\n",
    "use a built-in search tool via Tavily. However, it is really easy to create your\n",
    "own tools - see documentation\n",
    "[here](https://js.langchain.com/docs/modules/agents/tools/dynamic) on how to do\n",
    "that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "\n",
    "const tools = [new TavilySearchResults({ maxResults: 3 })];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Execution Agent\n",
    "\n",
    "Now we will create the execution agent we want to use to execute tasks. Note\n",
    "that for this example, we will be using the same execution agent for each task,\n",
    "but this doesn't HAVE to be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { pull } from \"langchain/hub\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { createOpenAIFunctionsAgent } from \"langchain/agents\";\n",
    "// Get the prompt to use - you can modify this!\n",
    "const prompt = await pull<ChatPromptTemplate>(\n",
    "  \"hwchase17/openai-functions-agent\",\n",
    ");\n",
    "// Choose the LLM that will drive the agent\n",
    "const llm = new ChatOpenAI({ modelName: \"gpt-4-0125-preview\" });\n",
    "// Construct the OpenAI Functions agent\n",
    "const agentRunnable = await createOpenAIFunctionsAgent({\n",
    "  llm,\n",
    "  tools,\n",
    "  prompt,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createAgentExecutor } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "const agentExecutor = createAgentExecutor({\n",
    "  agentRunnable,\n",
    "  tools,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  input: \u001b[32m\"who is the winner of the us open\"\u001b[39m,\n",
       "  agentOutcome: {\n",
       "    returnValues: {\n",
       "      output: \u001b[32m\"The winner of the 2023 US Open is Wyndham Clark. He won the 123rd edition of the major, holding off \"\u001b[39m... 398 more characters\n",
       "    },\n",
       "    log: \u001b[32m\"The winner of the 2023 US Open is Wyndham Clark. He won the 123rd edition of the major, holding off \"\u001b[39m... 398 more characters\n",
       "  },\n",
       "  steps: [\n",
       "    {\n",
       "      action: {\n",
       "        tool: \u001b[32m\"tavily_search_results_json\"\u001b[39m,\n",
       "        toolInput: { input: \u001b[32m\"US Open winner 2023\"\u001b[39m },\n",
       "        log: \u001b[32m'Invoking \"tavily_search_results_json\" with {\"input\":\"US Open winner 2023\"}\\n'\u001b[39m,\n",
       "        messageLog: [ \u001b[36m[AIMessage]\u001b[39m ]\n",
       "      },\n",
       "      observation: \u001b[32m`[{\"title\":\"Wyndham Clark wins 2023 US Open, clinching American's first ... - CNN\",\"url\":\"https://www`\u001b[39m... 3219 more characters\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await agentExecutor.invoke({ input: \"who is the winner of the us open\" });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the State\n",
    "\n",
    "Let's now start by defining the state the track for this agent.\n",
    "\n",
    "First, we will need to track the current plan. Let's represent that as a list of\n",
    "strings.\n",
    "\n",
    "Next, we should track previously executed steps. Let's represent that as a list\n",
    "of tuples (these tuples will contain the step and then the result)\n",
    "\n",
    "Finally, we need to have some state to represent the final response as well as\n",
    "the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const planExecuteState = {\n",
    "  input: {\n",
    "    value: null,\n",
    "  },\n",
    "  plan: {\n",
    "    value: null,\n",
    "    default: () => [],\n",
    "  },\n",
    "  pastSteps: {\n",
    "    value: (x, y) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "  response: {\n",
    "    value: null,\n",
    "  },\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning Step\n",
    "\n",
    "Let's now think about creating the planning step. This will use function calling\n",
    "to create a plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "import { zodToJsonSchema } from \"zod-to-json-schema\";\n",
    "\n",
    "const plan = zodToJsonSchema(z.object({\n",
    "  steps: z.array(z.string()).describe(\n",
    "    \"different steps to follow, should be in sorted order\",\n",
    "  ),\n",
    "}));\n",
    "const planFunction = {\n",
    "  name: \"plan\",\n",
    "  description: \"This tool is used to plan the steps to follow\",\n",
    "  parameters: plan,\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\n",
    "\n",
    "const plannerPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "{objective}`,\n",
    ");\n",
    "const model = new ChatOpenAI({\n",
    "  modelName: \"gpt-4-0125-preview\",\n",
    "}).bind({\n",
    "  functions: [planFunction],\n",
    "  function_call: planFunction,\n",
    "});\n",
    "const parserSingle = new JsonOutputFunctionsParser({ argsOnly: true });\n",
    "const planner = plannerPrompt.pipe(model).pipe(parserSingle);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  steps: [\n",
       "    \u001b[32m\"Identify the current Australia Open winner.\"\u001b[39m,\n",
       "    \u001b[32m\"Search for the winner's biography or profile.\"\u001b[39m,\n",
       "    \u001b[32m\"Locate the hometown information in the profile.\"\u001b[39m,\n",
       "    \u001b[32m\"Report the hometown of the current Australia Open winner.\"\u001b[39m\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await planner.invoke({\n",
    "  objective: \"what is the hometown of the current Australia open winner?\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Plan Step\n",
    "\n",
    "Now, let's create a step that re-does the plan based on the result of the\n",
    "previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createOpenAIFnRunnable } from \"langchain/chains/openai_functions\";\n",
    "import { JsonOutputFunctionsParser } from \"langchain/output_parsers\";\n",
    "\n",
    "const response = zodToJsonSchema(z.object({\n",
    "  response: z.string().describe(\"Response to user.\"),\n",
    "}));\n",
    "const responseFunction = {\n",
    "  name: \"response\",\n",
    "  description: \"Response to user.\",\n",
    "  parameters: response,\n",
    "};\n",
    "const replannerPrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `For the given objective, come up with a simple step by step plan.\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{pastSteps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that and use the 'response' function.\n",
    "Otherwise, fill out the plan.\n",
    "Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.`,\n",
    ");\n",
    "const parser = new JsonOutputFunctionsParser();\n",
    "const replanner = createOpenAIFnRunnable({\n",
    "  functions: [planFunction, responseFunction],\n",
    "  outputParser: parser,\n",
    "  llm: new ChatOpenAI({\n",
    "    modelName: \"gpt-4-0125-preview\",\n",
    "  }),\n",
    "  prompt: replannerPrompt,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Graph\n",
    "\n",
    "We can now create the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "type PlanExecuteState = {\n",
    "  input: string | null;\n",
    "  plan: Array<string>;\n",
    "  pastSteps: Array<string>;\n",
    "  response: string | null;\n",
    "};\n",
    "\n",
    "async function executeStep(\n",
    "  state: PlanExecuteState,\n",
    "): Promise<Partial<PlanExecuteState>> {\n",
    "  const task = state.input;\n",
    "  const agentResponse = await agentExecutor.invoke({ input: task });\n",
    "  return { pastSteps: [task, agentResponse.agentOutcome.returnValues.output] };\n",
    "}\n",
    "\n",
    "async function planStep(\n",
    "  state: PlanExecuteState,\n",
    "): Promise<Partial<PlanExecuteState>> {\n",
    "  const plan = await planner.invoke({ objective: state.input });\n",
    "  return { plan: plan.steps };\n",
    "}\n",
    "\n",
    "async function replanStep(\n",
    "  state: PlanExecuteState,\n",
    "): Promise<Partial<PlanExecuteState>> {\n",
    "  const output = await replanner.invoke({\n",
    "    input: state.input,\n",
    "    plan: state.plan ? state.plan.join(\"\\n\") : \"\",\n",
    "    pastSteps: state.pastSteps.join(\"\\n\"),\n",
    "  });\n",
    "  if (\"response\" in output) {\n",
    "    return { response: output.response };\n",
    "  }\n",
    "\n",
    "  return { plan: output.steps };\n",
    "}\n",
    "\n",
    "function shouldEnd(state: PlanExecuteState) {\n",
    "  if (state.response) {\n",
    "    return \"true\";\n",
    "  }\n",
    "  return \"false\";\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const workflow = new StateGraph({\n",
    "  channels: planExecuteState,\n",
    "});\n",
    "\n",
    "// Add the plan node\n",
    "workflow.addNode(\"planner\", planStep);\n",
    "\n",
    "// Add the execution step\n",
    "workflow.addNode(\"agent\", executeStep);\n",
    "\n",
    "// Add a replan node\n",
    "workflow.addNode(\"replan\", replanStep);\n",
    "\n",
    "workflow.setEntryPoint(\"planner\");\n",
    "\n",
    "// From plan we go to agent\n",
    "workflow.addEdge(\"planner\", \"agent\");\n",
    "\n",
    "// From agent, we replan\n",
    "workflow.addEdge(\"agent\", \"replan\");\n",
    "\n",
    "workflow.addConditionalEdges(\n",
    "  \"replan\",\n",
    "  // Next, we pass in the function that will determine which node is called next.\n",
    "  shouldEnd,\n",
    "  {\n",
    "    \"true\": END,\n",
    "    \"false\": \"planner\",\n",
    "  },\n",
    ");\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable,\n",
    "// meaning you can use it as you would any other runnable\n",
    "const app = workflow.compile();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  planner: {\n",
      "    plan: [\n",
      "      \"Check for the most recent updates on the 2024 Australia Open winner.\",\n",
      "      \"Identify the winner of the 2024 Australia Open.\",\n",
      "      \"Research the winner's biography for information on their hometown.\",\n",
      "      \"Report the hometown of the 2024 Australia Open winner.\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  agent: {\n",
      "    pastSteps: [\n",
      "      \"what is the hometown of the 2024 Australia open winner?\",\n",
      "      \"The 2024 Australian Open winner, Jannik Sinner, is from Italy. He became the first Italian man to wi\"... 33 more characters\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "{\n",
      "  replan: {\n",
      "    response: \"The 2024 Australian Open winner's hometown is not directly provided; however, it is known that Janni\"... 156 more characters\n",
      "  }\n",
      "}\n",
      "{\n",
      "  __end__: {\n",
      "    input: \"what is the hometown of the 2024 Australia open winner?\",\n",
      "    plan: [\n",
      "      \"Check for the most recent updates on the 2024 Australia Open winner.\",\n",
      "      \"Identify the winner of the 2024 Australia Open.\",\n",
      "      \"Research the winner's biography for information on their hometown.\",\n",
      "      \"Report the hometown of the 2024 Australia Open winner.\"\n",
      "    ],\n",
      "    pastSteps: [\n",
      "      \"what is the hometown of the 2024 Australia open winner?\",\n",
      "      \"The 2024 Australian Open winner, Jannik Sinner, is from Italy. He became the first Italian man to wi\"... 33 more characters\n",
      "    ],\n",
      "    response: \"The 2024 Australian Open winner's hometown is not directly provided; however, it is known that Janni\"... 156 more characters\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "const config = { recursionLimit: 50 };\n",
    "const inputs = {\n",
    "  input: \"what is the hometown of the 2024 Australia open winner?\",\n",
    "};\n",
    "\n",
    "for await (const event of await app.stream(inputs, config)) {\n",
    "  console.log(event);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### See the LangSmith trace [here](https://smith.langchain.com/public/276be79a-3016-4434-83c6-34715b942368/r)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
