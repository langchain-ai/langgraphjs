{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "In this quickstart guide, you'll get up and running with a simple agent that can\n",
    "search the web using [Tavily Search API](https://tavily.com/). The code is fully\n",
    "configurable, meaning you can swap out components, customize the execution flow,\n",
    "and extend to add any custom code or tooling.\n",
    "\n",
    "First install the required dependencies:\n",
    "\n",
    "```bash\n",
    "npm install @langchain/langgraph @langchain/openai @langchain/community\n",
    "```\n",
    "\n",
    "Then set the required environment variables. Optionally, set up\n",
    "[LangSmith](https://docs.smith.langchain.com/) for best-in-class observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quickstart: LangGraphJS\n"
     ]
    }
   ],
   "source": [
    "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
    "// process.env.TAVILY_API_KEY = \"sk_...\";\n",
    "\n",
    "// Optional, add tracing in LangSmith\n",
    "process.env.LANGCHAIN_API_KEY = \"ls__...\";\n",
    "process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n",
    "process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
    "process.env.LANGCHAIN_PROJECT = \"Quickstart: LangGraphJS\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in San Francisco is as follows:\n",
      "- Temperature: 82.9째F (28.3째C)\n",
      "- Condition: Sunny\n",
      "- Wind: 20.2 km/h from the NW\n",
      "- Humidity: 41%\n",
      "- Visibility: 9.0 miles\n",
      "- UV Index: 5.0\n",
      "\n",
      "For more details, you can visit [Weather in San Francisco](https://www.weatherapi.com/).\n",
      "The current weather in New York is as follows:\n",
      "- Temperature: 72.0째F (22.2째C)\n",
      "- Condition: Partly cloudy\n",
      "- Wind: 3.6 km/h from the N\n",
      "- Humidity: 57%\n",
      "- Cloud Cover: 75%\n",
      "- UV Index: 5.0\n",
      "\n",
      "For more details, you can visit [Weather in New York](https://www.weatherapi.com/).\n"
     ]
    }
   ],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "// Define the tools for the agent to use\n",
    "const agentTools = [new TavilySearchResults({ maxResults: 1 })];\n",
    "const agentModel = new ChatOpenAI({ temperature: 0 });\n",
    "\n",
    "// Initialize memory to persist state between graph runs\n",
    "const agentCheckpointer = new MemorySaver();\n",
    "const agent = createReactAgent({\n",
    "  llm: agentModel,\n",
    "  tools: agentTools,\n",
    "  checkpointSaver: agentCheckpointer,\n",
    "});\n",
    "\n",
    "// Now it's time to use!\n",
    "const agentFinalState = await agent.invoke(\n",
    "  { messages: [new HumanMessage(\"what is the weather in sf\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(\n",
    "  agentFinalState.messages[agentFinalState.messages.length - 1].content,\n",
    ");\n",
    "\n",
    "const agentNextState = await agent.invoke(\n",
    "  { messages: [new HumanMessage(\"what about ny\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(\n",
    "  agentNextState.messages[agentNextState.messages.length - 1].content,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n",
    "The\n",
    "[createReactAgent](https://langchain-ai.github.io/langgraphjs/reference/functions/prebuilt.createReactAgent.html)\n",
    "constructor lets you create a simple tool-using LangGraph agent in a single line\n",
    "of code. This can be powerful, but what makes LangGraph powerful is that you can\n",
    "fully decompose the agent for finer-grained control over its behavior. The\n",
    "following code creates an agent with the same behavior as the example above, but\n",
    "you can clearly see the execution logic and how you could customize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n",
    "import { ChatOpenAI } from \"@langchain/openai\";\n",
    "import { MemorySaver } from \"@langchain/langgraph\";\n",
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { END, START, StateGraph, StateGraphArgs } from \"@langchain/langgraph\";\n",
    "\n",
    "// Define the state interface\n",
    "interface AgentState {\n",
    "  messages: HumanMessage[];\n",
    "}\n",
    "\n",
    "// Define the graph state\n",
    "const graphState: StateGraphArgs<AgentState>[\"channels\"] = {\n",
    "  messages: {\n",
    "    value: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),\n",
    "    default: () => [],\n",
    "  },\n",
    "};\n",
    "\n",
    "// Define the tools for the agent to use\n",
    "const tools = [new TavilySearchResults({ maxResults: 1 })];\n",
    "\n",
    "const toolNode = new ToolNode<AgentState>(tools);\n",
    "\n",
    "const model = new ChatOpenAI({ temperature: 0 }).bindTools(tools);\n",
    "\n",
    "// Define the function that determines whether to continue or not\n",
    "function shouldContinue(state: AgentState): \"tools\" | typeof END {\n",
    "  const messages = state.messages;\n",
    "\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "\n",
    "  // If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "  if (lastMessage.additional_kwargs.tool_calls) {\n",
    "    return \"tools\";\n",
    "  }\n",
    "  // Otherwise, we stop (reply to the user)\n",
    "  return END;\n",
    "}\n",
    "\n",
    "// Define the function that calls the model\n",
    "async function callModel(state: AgentState) {\n",
    "  const messages = state.messages;\n",
    "\n",
    "  const response = await model.invoke(messages);\n",
    "\n",
    "  // We return a list, because this will get added to the existing list\n",
    "  return { messages: [response] };\n",
    "}\n",
    "\n",
    "// Define a new graph\n",
    "const workflow = new StateGraph<AgentState>({ channels: graphState })\n",
    "  .addNode(\"agent\", callModel)\n",
    "  .addNode(\"tools\", toolNode)\n",
    "  .addEdge(START, \"agent\")\n",
    "  .addConditionalEdges(\"agent\", shouldContinue)\n",
    "  .addEdge(\"tools\", \"agent\");\n",
    "\n",
    "// Initialize memory to persist state between graph runs\n",
    "const checkpointer = new MemorySaver();\n",
    "\n",
    "// Finally, we compile it!\n",
    "// This compiles it into a LangChain Runnable.\n",
    "// Note that we're (optionally) passing the memory when compiling the graph\n",
    "const app = workflow.compile({ checkpointer });\n",
    "\n",
    "// Use the agent\n",
    "const finalState = await app.invoke(\n",
    "  { messages: [new HumanMessage(\"what is the weather in sf\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(finalState.messages[finalState.messages.length - 1].content);\n",
    "\n",
    "const nextState = await app.invoke(\n",
    "  { messages: [new HumanMessage(\"what about ny\")] },\n",
    "  { configurable: { thread_id: \"42\" } },\n",
    ");\n",
    "\n",
    "console.log(nextState.messages[nextState.messages.length - 1].content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've created a custom agent, check out the\n",
    "[tutorials](https://langchain-ai.github.io/langgraphjs/tutorials/) to learn\n",
    "LangGraph by implementing different types of end-to-end workflows from RAG to\n",
    "multi-agent swarms, or consult the\n",
    "[how-to guides](https://langchain-ai.github.io/langgraphjs/how-tos/) for more\n",
    "examples of how to implement different design patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TypeScript",
   "language": "typescript",
   "name": "tslab"
  },
  "language_info": {
   "codemirror_mode": {
    "mode": "typescript",
    "name": "javascript",
    "typescript": true
   },
   "file_extension": ".ts",
   "mimetype": "text/typescript",
   "name": "typescript",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
