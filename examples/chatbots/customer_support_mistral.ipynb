{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer support chatbot\n",
    "\n",
    "Below is an example of a customer support chatbot modeled as a state machine. It uses the simpler `MessageGraph` version of LangGraph, and is designed to work with smaller models by reducing the decision space a given LLM call has.\n",
    "\n",
    "The entrypoint is a node containing a chain that we have prompted to answer basic questions, but delegate questions related to billing or technical support to other \"teams\".\n",
    "\n",
    "Depending on this entry node's response, the edge from that node will use an LLM call to determine whether to respond directly to the user or invoke either the `billing_support` or `technical_support` nodes.\n",
    "\n",
    "- The technical support will attempt to answer the user's question with a more focused prompt.\n",
    "- The billing agent can choose to answer the user's question, or can authorize a refund (currently just returns directly to the user with an acknowledgement).\n",
    "\n",
    "![Diagram](./diagram.png)\n",
    "\n",
    "This is intended as a sample, proof of concept architecture - you could extend this example by giving individual nodes the ability to perform retrieval, other tools, adding human-in-the-loop/prompting the user for responses, delegating to more powerful models at deeper stages etc.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we need to install the required packages. We'll use Cloudflare's Workers AI to run the required inference.\n",
    "\n",
    "```bash\n",
    "yarn add @langchain/langgraph @langchain/cloudflare\n",
    "```\n",
    "\n",
    "You'll also need to set the following environment variable. You can get them from your Cloudflare dashboard:\n",
    "\n",
    "```ini\n",
    "CLOUDFLARE_ACCOUNT_ID=\n",
    "CLOUDFLARE_API_TOKEN=\n",
    "```\n",
    "\n",
    "## Initializing the model\n",
    "\n",
    "First, we define the LLM we'll use for all calls and the LangGraph state. We'll use a chat fine-tuned version of Mistral 7B called `neural-chat-7b-v3-1-awq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatCloudflareWorkersAI } from \"@langchain/cloudflare\";\n",
    "import { MessageGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "const model = new ChatCloudflareWorkersAI({\n",
    "  model: \"@hf/thebloke/neural-chat-7b-v3-1-awq\",\n",
    "  temperature: 0,\n",
    "});\n",
    "\n",
    "const graph = new MessageGraph();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise, let's see what happens with a naive attempt to get the model to answer questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangCorp is a company that specializes in selling computers and related accessories. They offer a wide range of products, including laptops, desktops, monitors, keyboards, mice, and other peripherals. Their goal is to provide customers with high-quality, reliable, and affordable technology solutions to meet their needs.\n",
      "\n",
      "LangCorp's team of experts is always ready to assist customers in finding the perfect computer setup for their requirements. They offer personalized advice, help with configuration, and provide support throughout the entire purchasing process. Additionally, LangCorp ensures that their products are backed by warranties and after-sales services to guarantee customer satisfaction.\n",
      "\n",
      "In summary, LangCorp is a company that focuses on selling computers and related accessories, aiming to provide customers with the best technology solutions for their needs. They offer a variety of products, expert advice, and excellent customer support to ensure a seamless shopping experience.\n"
     ]
    }
   ],
   "source": [
    "import { ChatPromptTemplate } from \"@langchain/core/prompts\";\n",
    "import { StringOutputParser } from \"@langchain/core/output_parsers\";\n",
    "\n",
    "const naivePrompt = ChatPromptTemplate.fromTemplate(\n",
    "  `You are an expert support specialist, able to answer any question about LangCorp, a company that sells computers.`\n",
    ");\n",
    "\n",
    "const chain = naivePrompt.pipe(model).pipe(new StringOutputParser());\n",
    "\n",
    "const res = await chain.invoke(\"I've changed my mind and I want a refund for order #182818!\");\n",
    "\n",
    "console.log(res);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super helpful. We can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laying out the graph\n",
    "\n",
    "Now let's start defining our nodes. Each node's return value will be added to the graph state, which for `MessageGraph` is a list of messages. This state will be passed to the next executed node, or returned if execution has finished.\n",
    "\n",
    "Let's define our entrypoint node. This will be modeled after a secretary who can handle incoming questions and respond conversationally or route to a more specialized team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import type { BaseMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "graph.addNode(\"initial_support\", async (state: BaseMessage[]) => {\n",
    "  const SYSTEM_TEMPLATE = `You are frontline support staff for LangCorp, a company that sells computers.\n",
    "Be concise in your responses.\n",
    "You can chat with customers and help them with basic questions, but if the customer is having a billing or technical problem,\n",
    "do not try to answer the question directly or gather information.\n",
    "Instead, immediately transfer them to the billing or technical team by asking the user to hold for a moment.\n",
    "Otherwise, just respond conversationally.`;\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", SYSTEM_TEMPLATE],\n",
    "    new MessagesPlaceholder(\"messages\"),\n",
    "  ]);\n",
    "\n",
    "  return prompt.pipe(model).invoke({ messages: state });\n",
    "});\n",
    "\n",
    "graph.setEntryPoint(\"initial_support\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our nodes representing billing and technical support. We give special instructions in the billing prompt that it can choose to authorize refunds by routing to another agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.addNode(\"billing_support\", async (state: BaseMessage[]) => {\n",
    "  const SYSTEM_TEMPLATE = `You are an expert billing support specialist for LangCorp, a company that sells computers.\n",
    "Help the user to the best of your ability, but be concise in your responses.\n",
    "You have the ability to authorize refunds, which you can do by transferring the user to another agent who will collect the required information.\n",
    "If you do, assume the other agent has all necessary information about the customer and their order.\n",
    "You do not need to ask the user for more information.`;\n",
    "\n",
    "  let messages = state;\n",
    "  // Make the user's question the most recent message in the history.\n",
    "  // This helps small models stay focused.\n",
    "  if (messages[messages.length - 1]._getType() === \"ai\") {\n",
    "    messages = state.slice(0, -1);\n",
    "  }\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", SYSTEM_TEMPLATE],\n",
    "    new MessagesPlaceholder(\"messages\"),\n",
    "  ]);\n",
    "  return prompt.pipe(model).invoke({ messages });\n",
    "});\n",
    "\n",
    "graph.addNode(\"technical_support\", async (state: BaseMessage[]) => {\n",
    "  const SYSTEM_TEMPLATE = `You are an expert at diagnosing technical computer issues. You work for a company called LangCorp that sells computers.\n",
    "Help the user to the best of your ability, but be concise in your responses.`;\n",
    "\n",
    "  let messages = state;\n",
    "  // Make the user's question the most recent message in the history.\n",
    "  // This helps small models stay focused.\n",
    "  if (messages[messages.length - 1]._getType() === \"ai\") {\n",
    "    messages = state.slice(0, -1);\n",
    "  }\n",
    "\n",
    "  const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", SYSTEM_TEMPLATE],\n",
    "    new MessagesPlaceholder(\"messages\"),\n",
    "  ]);\n",
    "  return prompt.pipe(model).invoke({ messages });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a node that can handle refunds. The logic is stubbed out here since it's not a real system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { AIMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "graph.addNode(\"refund_tool\", async (state) => {\n",
    "  return new AIMessage(\"Refund processed!\");\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the nodes\n",
    "\n",
    "Great! Now let's move onto the edges. These edges will evaluate the current state of the graph created by the return values of the individual nodes and route execution accordingly.\n",
    "\n",
    "First, we want our `initial_support` node to either delegate to the billing node, technical node, or just respond directly to the user. Here's one example of how we might do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { END } from \"@langchain/langgraph\";\n",
    "\n",
    "\n",
    "graph.addConditionalEdges(\"initial_support\", async (state) => {\n",
    "  const mostRecentMessage = state[state.length - 1];\n",
    "  const SYSTEM_TEMPLATE = `You are an expert customer support routing system.\n",
    "Your job is to detect whether a customer support representative is routing a user to a billing team or a technical team, or if they are just responding conversationally.`;\n",
    "  const HUMAN_TEMPLATE = `The previous conversation is an interaction between a customer support representative and a user.\n",
    "Extract whether the representative is routing the user to a billing or technical team, or whether they are just responding conversationally.\n",
    "\n",
    "If they want to route the user to the billing team, respond only with the word \"BILLING\".\n",
    "If they want to route the user to the technical team, respond only with the word \"TECHNICAL\".\n",
    "Otherwise, respond only with the word \"RESPOND\".\n",
    "\n",
    "Remember, only respond with one of the above words.`;\n",
    "  const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", SYSTEM_TEMPLATE],\n",
    "    new MessagesPlaceholder(\"messages\"),\n",
    "    [\"human\", HUMAN_TEMPLATE],\n",
    "  ]);\n",
    "  const chain = prompt\n",
    "    .pipe(model)\n",
    "    .pipe(new StringOutputParser());\n",
    "  const rawCategorization = await chain.invoke({ messages: state });\n",
    "  if (rawCategorization.includes(\"BILLING\")) {\n",
    "    return \"billing\";\n",
    "  } else if (rawCategorization.includes(\"TECHNICAL\")) {\n",
    "    return \"technical\";\n",
    "  } else {\n",
    "    return \"conversational\";\n",
    "  }\n",
    "}, {\n",
    "  billing: \"billing_support\",\n",
    "  technical: \"technical_support\",\n",
    "  conversational: END\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We do not use function/tool calling here for extraction because our model does not support it, but this would be a reasonable time to use that if your model does.\n",
    "\n",
    "Let's continue. We add an edge making the technical support node always end, since it has no tools to call. The billing support node uses a conditional edge since it can either call the refund tool or end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.addEdge(\"technical_support\", END);\n",
    "\n",
    "graph.addConditionalEdges(\"billing_support\", async (state) => {\n",
    "  const mostRecentMessage = state[state.length - 1];\n",
    "  const SYSTEM_TEMPLATE = `Your job is to detect whether a billing support representative wants to refund the user.`;\n",
    "  const HUMAN_TEMPLATE = `The following text is a response from a customer support representative.\n",
    "Extract whether they want to refund the user or not.\n",
    "If they want to refund the user, respond only with the word \"REFUND\".\n",
    "Otherwise, respond only with the word \"RESPOND\".\n",
    "\n",
    "Here is the text:\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "Remember, only respond with one word.`;\n",
    "  const prompt = ChatPromptTemplate.fromMessages([\n",
    "    [\"system\", SYSTEM_TEMPLATE],\n",
    "    [\"human\", HUMAN_TEMPLATE],\n",
    "  ]);\n",
    "  const chain = prompt\n",
    "    .pipe(model)\n",
    "    .pipe(new StringOutputParser());\n",
    "  const response = await chain.invoke({ text: mostRecentMessage.content });\n",
    "  if (response.includes(\"REFUND\")) {\n",
    "    return \"refund\";\n",
    "  } else {\n",
    "    return \"end\";\n",
    "  }\n",
    "}, {\n",
    "  refund: \"refund_tool\",\n",
    "  end: END\n",
    "});\n",
    "\n",
    "graph.addEdge(\"refund_tool\", END);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lock it in by calling `.compile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const runnable = graph.compile();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's test it!\n",
    "\n",
    "We can get the returned value from the executed nodes as they are generated using the `.stream()` runnable method (we also could go even more granular and get output as it is generated using `.streamEvents()`, but this requires a bit more parsing).\n",
    "\n",
    "Here's an example with a billing related refund query. Because we are using `MessageGraph`, the input must be a message (or a list of messages) representing the user's question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STEP---\n",
      "initial_support To request a refund for order #182818, please hold for a moment while I transfer you to our billing team.\n",
      "---END STEP---\n",
      "---STEP---\n",
      "billing_support To process your refund, please transfer the call to our refunds team. They will guide you through the necessary steps.\n",
      "---END STEP---\n",
      "---STEP---\n",
      "refund_tool Refund processed!\n",
      "---END STEP---\n"
     ]
    }
   ],
   "source": [
    "import { HumanMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "const stream = await runnable.stream(\n",
    "  new HumanMessage(\"I've changed my mind and I want a refund for order #182818!\")\n",
    ");\n",
    "\n",
    "for await (const value of stream) {\n",
    "  // Each node returns only one message\n",
    "  const [nodeName, output] = Object.entries(value)[0];\n",
    "  if (nodeName !== END) {\n",
    "    console.log(\"---STEP---\")\n",
    "    console.log(nodeName, output.content);\n",
    "    console.log(\"---END STEP---\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/08fb80d9-4ec2-4460-a62d-f7fdc1a21f96/r)\n",
    "\n",
    "Now, let's try a technical question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STEP---\n",
      "initial_support I'm sorry to hear that. Please hold for a moment while I transfer you to our technical team who can help you with this issue.\n",
      "---END STEP---\n",
      "---STEP---\n",
      "technical_support Unfortunately, your computer is likely damaged beyond repair due to water exposure. You should consider purchasing a new one. Contact LangCorp for assistance if needed.\n",
      "\n",
      "Note: Always seek professional help for water-damaged electronics. Drying them out may not be enough to fix the issue.\n",
      "---END STEP---\n"
     ]
    }
   ],
   "source": [
    "const stream = await runnable.stream(\n",
    "  new HumanMessage(\"My LangCorp computer isn't turning on because I dropped it in water.\")\n",
    ");\n",
    "\n",
    "for await (const value of stream) {\n",
    "  // Each node returns only one message\n",
    "  const [nodeName, output] = Object.entries(value)[0];\n",
    "  if (nodeName !== END) {\n",
    "    console.log(\"---STEP---\")\n",
    "    console.log(nodeName, output.content);\n",
    "    console.log(\"---END STEP---\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/787fd20a-dea8-426b-bfb2-ebb0aed21505/r)\n",
    "\n",
    "We can see the query gets correctly routed to the technical support node!\n",
    "\n",
    "Finally, let's try a simple conversational response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---STEP---\n",
      "initial_support Hi Cobb, I'm doing well. How can I help you today?\n",
      "---END STEP---\n"
     ]
    }
   ],
   "source": [
    "const stream = await runnable.stream(\n",
    "  new HumanMessage(\"How are you? I'm Cobb.\")\n",
    ");\n",
    "\n",
    "for await (const value of stream) {\n",
    "  // Each node returns only one message\n",
    "  const [nodeName, output] = Object.entries(value)[0];\n",
    "  if (nodeName !== END) {\n",
    "    console.log(\"---STEP---\")\n",
    "    console.log(nodeName, output.content);\n",
    "    console.log(\"---END STEP---\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to see a LangSmith trace of the above run](https://smith.langchain.com/public/095dd3af-19d6-4377-95f3-4c219c47a87d/r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
