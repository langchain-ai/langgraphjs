{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
      "metadata": {},
      "source": [
       "# Chat Bot Evaluation as Multi-agent Simulation\n",
       "\n",
       "When building a chat bot, such as a customer support assistant, it can be hard to properly evaluate your bot's performance. It's time-consuming to have to manually interact with it intensively for each code change.\n",
       "\n",
       "One way to make the evaluation process easier and more reproducible is to simulate a user interaction.\n",
       "\n",
       "With LangGraph, it's easy to set this up. Below is an example of how to create a \"virtual user\" to simulate a conversation.\n",
       "\n",
       "The overall simulation looks something like this:\n",
       "\n",
       "![diagram](./img/virtual_user_diagram.png)\n",
       "\n",
       "First, we'll set up our environment."
      ]
     },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ed4de09-9ee7-4a2a-bcc7-54236e7cccdf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Agent Simulation Evaluation: LangGraphJS'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "// process.env.OPENAI_API_KEY = \"sk_...\";\n",
        "// Optional tracing in LangSmith\n",
        "// process.env.LANGCHAIN_API_KEY = \"sk_...\";\n",
        "// process.env.LANGCHAIN_TRACING_V2 = \"true\";\n",
        "// process.env.LANGCHAIN_PROJECT = \"Agent Simulation Evaluation: LangGraphJS\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3febb04f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import {\n",
        "  AIMessage,\n",
        "  AIMessageChunk,\n",
        "  BaseMessage,\n",
        "  HumanMessage,\n",
        "} from '@langchain/core/messages'\n",
        "import {\n",
        "  ChatPromptTemplate,\n",
        "  MessagesPlaceholder,\n",
        "} from '@langchain/core/prompts'\n",
        "import { Runnable } from '@langchain/core/runnables'\n",
        "import { Annotation, END, START, StateGraph } from '@langchain/langgraph'\n",
        "import { ChatOpenAI } from '@langchain/openai'\n",
        "\n",
        "const llm = new ChatOpenAI({\n",
        "  apiKey: process.env.OPENAI_API_KEY,\n",
        "});"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ef8e26",
      "metadata": {},
      "source": [
        "## 1. Define Chat Bot\n",
        "\n",
        "Next, we'll define our chat bot. This implementation uses the OpenAI API to generate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "afefe443",
      "metadata": {},
      "outputs": [],
      "source": [
        "interface Message {\n",
        "  role: string;\n",
        "  content: string;\n",
        "}\n",
        "\n",
        "async function myChatBot(messages: Message[]): Promise<AIMessageChunk> {\n",
        "  const systemMessage: Message = {\n",
        "    role: 'system',\n",
        "    content: 'You are a customer support agent for an airline.',\n",
        "  };\n",
        "  const allMessages = [systemMessage, ...messages];\n",
        "  \n",
        "  const response = await llm.invoke(\n",
        "    allMessages.map((m) => [m.role, m.content]),\n",
        "  )\n",
        "  return response\n",
        "}\n",
        "\n",
        "// Test the chat bot\n",
        "const response = await myChatBot([{role: 'user', content: 'hi!'}]);\n",
        "console.log(response);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5635e4",
      "metadata": {},
      "source": [
        "## 2. Define Simulated User\n",
        "\n",
        "Now we'll define the simulated user using LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e170c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "async function createSimulatedUser(): Promise<Runnable> {\n",
        "    const systemPromptTemplate = `You are a customer of an airline company. You are interacting with a user who is a customer support person \n",
        "    \n",
        "    {instructions}\n",
        "    \n",
        "    When you are finished with the conversation, respond with a single word \"FINISHED\"`;\n",
        "    \n",
        "    const prompt = ChatPromptTemplate.fromMessages([\n",
        "      ['system', systemPromptTemplate],\n",
        "      new MessagesPlaceholder('messages'),\n",
        "    ]);\n",
        "    \n",
        "    const instructions = \"Your name is Harrison. You are trying to get a refund for the trip you took to Alaska. \\\n",
        "    You want them to give you ALL the money back. This trip happened 5 years ago.\";\n",
        "    \n",
        "    const partialPrompt = await prompt.partial({ name: 'Harrison', instructions });\n",
        "    \n",
        "    const simulatedUser = await partialPrompt.pipe(model);\n",
        "    return simulatedUser\n",
        "}\n",
        "\n",
        "// Test the simulated user\n",
        "const messages = [new HumanMessage('Hi! How can I help you?')];\n",
        "const simulatedUser= await createSimulatedUser()\n",
        "const simulatedUserResponse = await simulatedUser.invoke({ messages });\n",
        "console.log(simulatedUserResponse);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "321312b4-a1f0-4454-a481-fdac4e37cb7d",
      "metadata": {},
      "source": [
        "## 3. Define the Agent Simulation\n",
        "\n",
        "The code below creates a LangGraph workflow to run the simulation. The main components are:\n",
        "\n",
        "1. The two nodes: one for the simulated user, the other for the chat bot.\n",
        "2. The graph itself, with a conditional stopping criterion.\n",
        "\n",
        "Read the comments in the code below for more information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65bc4446-462b-4ee8-b017-2862fbbdfaf5",
      "metadata": {},
      "source": [
        "**Nodes**\n",
        "\n",
        "First, we define the nodes in the graph. These should take in a list of messages and return a list of messages to ADD to the state.\n",
        "These will be thing wrappers around the chat bot and simulated user we have above.\n",
        "\n",
        "**Note:** one tricky thing here is which messages are which. Because both the chat bot AND our simulated user are both LLMs, both of them will resond with AI messages. Our state will be a list of alternating Human and AI messages. This means that for one of the nodes, there will need to be some logic that flips the AI and human roles. In this example, we will assume that HumanMessages are messages from the simulated user. This means that we need some logic in the simulated user node to swap AI and Human messages.\n",
        "\n",
        "First, let's define the chat bot node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d5d9e0-41ee-46c9-b62f-9128b91f99ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "function chatBotNode(state: { messages: any[] }): { messages: AIMessage[] } {\n",
        "  const messages = state.messages\n",
        "  const chatBotResponse = await myChatBot(\n",
        "    messages.map((m) => ({ role: m._getType(), content: m.content })),\n",
        "  )\n",
        "  return { messages: [new AIMessage({ content: chatBotResponse.content })] }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694c3c0c-56c5-4410-8fa8-ea2c0f11f506",
      "metadata": {},
      "source": [
        "Next, let's define the node for our simulated user. This will involve a little logic to swap the roles of the messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac45873-91a3-4310-939f-a0a53da4233d",
      "metadata": {},
      "outputs": [],
      "source": [
        "function swapRoles(messages: any[]): any[] {\n",
        "  return messages.map((m) =>\n",
        "    m instanceof AIMessage\n",
        "      ? new HumanMessage({ content: m.content })\n",
        "      : new AIMessage({ content: m.content }),\n",
        "  )\n",
        "}\n",
        "\n",
        "async function simulatedUserNode(state: { messages: any[] }): Promise<{\n",
        "  messages: HumanMessage[]\n",
        "}> {\n",
        "  const messages = state.messages\n",
        "  const newMessages = swapRoles(messages)\n",
        "  const simulateUser = await simulatedUser()\n",
        "  const response = await simulateUser.invoke({ messages: newMessages })\n",
        "\n",
        "  return { messages: [new HumanMessage({ content: response.content })] }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48d8a3e-9171-4c43-a595-44d312722148",
      "metadata": {},
      "source": [
        "**Edges**\n",
        "\n",
        "We now need to define the logic for the edges. The main logic occurs after the simulated user goes, and it should lead to one of two outcomes:\n",
        "\n",
        "- Either we continue and call the customer support bot\n",
        "- Or we finish and the conversation is over\n",
        "\n",
        "So what is the logic for the conversation being over? We will define that as either the Human chatbot responds with `FINISHED` (see the system prompt) OR the conversation is more than 6 messages long (this is an arbitrary number just to keep this example short)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820a1557-a121-48c2-af5b-81220f43c81b",
      "metadata": {},
      "outputs": [],
      "source": [
        "function shouldContinue(state: { messages: any[] }): string {\n",
        "  const messages = state.messages;\n",
        "  if (messages.length > 6) {\n",
        "    return 'end';\n",
        "  } else if (messages[messages.length - 1].content === 'FINISHED') {\n",
        "    return 'end';\n",
        "  } else {\n",
        "    return 'continue';\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0856d4f-9334-4f28-944b-06d303e913a4",
      "metadata": {},
      "source": [
        "**Graph**\n",
        "\n",
        "We can now define the graph that sets up the simulation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1889196d",
      "metadata": {},
      "outputs": [],
      "source": [
        "interface State {\n",
        "  messages: any[];\n",
        "}\n",
        "\n",
        "function createSimulation(){\n",
        "  const State = Annotation.Root({\n",
        "    messages: Annotation<BaseMessage[]>({\n",
        "      reducer: (x, y) => x.concat(y),\n",
        "      default: () => [],\n",
        "    }),\n",
        "  })\n",
        "\n",
        "  const workflow = new StateGraph(State)\n",
        "    .addNode('user', simulatedUserNode)\n",
        "    .addNode('chatbot', chatBotNode)\n",
        "    .addEdge('chatbot', 'user')\n",
        "    .addConditionalEdges('user', shouldContinue, {\n",
        "      end: END,\n",
        "      continue: 'chatbot',\n",
        "    })\n",
        "    .addEdge(START, 'chatbot')\n",
        "\n",
        "  const simulation = workflow.compile()\n",
        "  return simulation\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79fd22ce",
      "metadata": {},
      "source": [
        "## 4. Run Simulation\n",
        "\n",
        "Now we can evaluate our chat bot! We can invoke it with empty messages (this will simulate letting the chat bot start the initial conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32b606f",
      "metadata": {},
      "outputs": [],
      "source": [
        "async function runSimulation() {\n",
        "  const simulation = createSimulation()\n",
        "  for await (const chunk of await simulation.stream({})) {\n",
        "    console.log(chunk)\n",
        "    console.log('\\n---\\n')\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "await runSimulation();"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
