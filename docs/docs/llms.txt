# LangGraph

## Quickstart

These guides are designed to help you get started with LangGraph.

- [LangGraph Quickstart](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/): Build a chatbot that can use tools and keep track of conversation history. Add human-in-the-loop capabilities and explore how time-travel works.
- [Common Workflows](https://langchain-ai.github.io/langgraphjs/tutorials/workflows/): Overview of the most common workflows using LLMs implemented with LangGraph.
- [LangGraph Server Quickstart](https://langchain-ai.github.io/langgraphjs/tutorials/langgraph-platform/local-server/): Launch a LangGraph server locally and interact with it using REST API and LangGraph Studio Web UI.
- [Deploy with LangGraph Cloud Quickstart](https://langchain-ai.github.io/langgraphjs/cloud/quick_start/): Deploy a LangGraph app using LangGraph Cloud.

## Concepts

These guides provide explanations of the key concepts behind the LangGraph framework.

- [Why LangGraph?](https://langchain-ai.github.io/langgraphjs/concepts/high_level/): Motivation for LangGraph, a library for building agentic applications with LLMs.
- [LangGraph Glossary](https://langchain-ai.github.io/langgraphjs/concepts/low_level/): LangGraph workflows are designed as graphs, with nodes representing different components and edges representing the flow of information between them. This guide provides an overview of the key concepts associated with LangGraph graph primitives.
- [Common Agentic Patterns](https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/): An agent uses an LLM to pick its own control flow to solve more complex problems! Agents are a key building block in many LLM applications. This guide explains the different types of agent architectures and how they can be used to control the flow of an application.
- [Multi-Agent Systems](https://langchain-ai.github.io/langgraphjs/concepts/multi_agent/): Complex LLM applications can often be broken down into multiple agents, each responsible for a different part of the application. This guide explains common patterns for building multi-agent systems.
- [Breakpoints](https://langchain-ai.github.io/langgraphjs/concepts/breakpoints/): Breakpoints allow pausing the execution of a graph at specific points. Breakpoints allow stepping through graph execution for debugging purposes.
- [Human-in-the-Loop](https://langchain-ai.github.io/langgraphjs/concepts/human_in_the_loop/): Explains different ways of integrating human feedback into a LangGraph application.
- [Time Travel](https://langchain-ai.github.io/langgraphjs/concepts/time-travel/): Time travel allows you to replay past actions in your LangGraph application to explore alternative paths and debug issues.
- [Persistence](https://langchain-ai.github.io/langgraphjs/concepts/persistence/): LangGraph has a built-in persistence layer, implemented through checkpointers. This persistence layer helps to support powerful capabilities like human-in-the-loop, memory, time travel, and fault-tolerance.
- [Memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/): Memory in AI applications refers to the ability to process, store, and effectively recall information from past interactions. With memory, your agents can learn from feedback and adapt to users' preferences.
- [Streaming](https://langchain-ai.github.io/langgraphjs/concepts/streaming/): Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.
- [Functional API](https://langchain-ai.github.io/langgraphjs/concepts/functional_api/): `@entrypoint` and `@task` decorators that allow you to add LangGraph functionality to an existing codebase.
- [FAQ](https://langchain-ai.github.io/langgraphjs/concepts/faq/): Frequently asked questions about LangGraph.

## How-tos

Here you'll find answers to "How do I...?" types of questions. 

These guides are **goal-oriented** and concrete. 

They're meant to help you complete a specific task.

### Fine-grained Control

These guides demonstrate LangGraph features that grant fine-grained control over the execution of your graph.

- [How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraphjs/how-tos/map-reduce/)
- [How to update state and jump to nodes in graphs and subgraphs](https://langchain-ai.github.io/langgraphjs/how-tos/command/)
- [How to defer node execution](https://langchain-ai.github.io/langgraphjs/how-tos/defer-node-execution/)
- [How to add runtime configuration to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)

### Persistence
 
Persistence makes it easy to persist state across graph runs (per-thread persistence) and across threads (cross-thread persistence). 

These how-to guides show how to add persistence to your graph.

- [How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/)
- [How to add thread-level persistence to a subgraph](https://langchain-ai.github.io/langgraphjs/how-tos/subgraph-persistence/)
- [How to add cross-thread persistence to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/cross-thread-persistence/)
- [How to use Postgres checkpointer for persistence](https://langchain-ai.github.io/langgraphjs/how-tos/persistence_postgres/)

See the below guides for how-to add persistence to your workflow using the [Functional API](https://langchain-ai.github.io/langgraphjs/concepts/functional_api/):

- [How to add thread-level persistence (functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/persistence-functional/)
- [How to add cross-thread persistence (functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/cross-thread-persistence-functional/)

### Memory

LangGraph makes it easy to manage conversation memory in your graph. These how-to guides show how to implement different strategies for that.

- [How to manage conversation history](https://langchain-ai.github.io/langgraphjs/how-tos/memory/manage-conversation-history/)
- [How to delete messages](https://langchain-ai.github.io/langgraphjs/how-tos/memory/delete-messages/)
- [How to add summary conversation memory](https://langchain-ai.github.io/langgraphjs/how-tos/memory/add-summary-conversation-history/)
- [How to add long-term memory (cross-thread)](https://langchain-ai.github.io/langgraphjs/how-tos/memory/cross-thread-persistence/)
- [How to use semantic search for long-term memory](https://langchain-ai.github.io/langgraphjs/how-tos/memory/semantic-search/)

### Human-in-the-loop

Human-in-the-loop functionality allows you to involve humans in the decision-making process of your graph.

These how-to guides show how to implement human-in-the-loop workflows in your graph.

- [How to wait for user input](https://langchain-ai.github.io/langgraphjs/how-tos/human_in_the_loop/wait-user-input/): A basic example that shows how to implement a human-in-the-loop workflow in your graph using the `interrupt` function.
- [How to review tool calls](https://langchain-ai.github.io/langgraphjs/how-tos/human_in_the_loop/review-tool-calls/): Incorporate human-in-the-loop for reviewing/editing/accepting tool call requests before they executed using the `interrupt` function.
- [How to add static breakpoints](https://langchain-ai.github.io/langgraphjs/how-tos/human_in_the_loop/breakpoints/): Use for debugging purposes. For human-in-the-loop workflows, we recommend the [`interrupt` function](https://langchain-ai.github.io/langgraphjs/reference/types/#langgraph.types.interrupt) instead.
- [How to edit graph state](https://langchain-ai.github.io/langgraphjs/how-tos/human_in_the_loop/edit-graph-state/): Edit graph state using `graph.update_state` method. Use this if implementing a **human-in-the-loop** workflow via **static breakpoints**.

See the below guides for how-to implement human-in-the-loop workflows with the Functional API.

- [How to wait for user input (Functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/wait-user-input-functional/)
- [How to review tool calls (Functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/review-tool-calls-functional/)

### Time Travel

[Time travel](https://langchain-ai.github.io/langgraphjs/concepts/time-travel/) allows you to replay past actions in your LangGraph application to explore alternative paths and debug issues. These how-to guides show how to use time travel in your graph.

- [How to view and update past graph state](https://langchain-ai.github.io/langgraphjs/how-tos/time-travel/)

### Streaming

[Streaming](https://langchain-ai.github.io/langgraphjs/concepts/streaming/) is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

- [How to stream the full state of your graph](https://langchain-ai.github.io/langgraphjs/how-tos/stream-values/)
- [How to stream state updates of your graph](https://langchain-ai.github.io/langgraphjs/how-tos/stream-updates/)
- [How to stream LLM tokens](https://langchain-ai.github.io/langgraphjs/how-tos/stream-tokens/)
- [How to stream LLM tokens without LangChain models](https://langchain-ai.github.io/langgraphjs/how-tos/streaming-tokens-without-langchain/)
- [How to stream custom data](https://langchain-ai.github.io/langgraphjs/how-tos/streaming-content/)
- [How to configure multiple streaming modes](https://langchain-ai.github.io/langgraphjs/how-tos/stream-multiple/)
- [How to stream events from within a tool](https://langchain-ai.github.io/langgraphjs/how-tos/streaming-events-from-within-tools/)
- [How to stream from the final node](https://langchain-ai.github.io/langgraphjs/how-tos/streaming-from-final-node/)

### Tool calling

- [How to call tools using ToolNode](https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling/)
- [How to force an agent to call a tool](https://langchain-ai.github.io/langgraphjs/how-tos/force-calling-a-tool-first/)
- [How to handle tool calling errors](https://langchain-ai.github.io/langgraphjs/how-tos/tool-calling-errors/)
- [How to pass runtime values to tools](https://langchain-ai.github.io/langgraphjs/how-tos/pass-run-time-values-to-tools/)
- [How to update graph state from tools](https://langchain-ai.github.io/langgraphjs/how-tos/update-state-from-tools/)

### Subgraphs

[Subgraphs](https://langchain-ai.github.io/langgraphjs/concepts/low_level/#subgraphs) allow you to reuse an existing graph from another graph. These how-to guides show how to use subgraphs:

- [How to add and use subgraphs](https://langchain-ai.github.io/langgraphjs/how-tos/subgraph/)
- [How to view and update state in subgraphs](https://langchain-ai.github.io/langgraphjs/how-tos/subgraphs-manage-state/)
- [How to transform inputs and outputs of a subgraph](https://langchain-ai.github.io/langgraphjs/how-tos/subgraph-transform-state/)

### Multi-agent

- [How to build a multi-agent network](https://langchain-ai.github.io/langgraphjs/how-tos/multi-agent-network/)
- [How to add multi-turn conversation in a multi-agent application](https://langchain-ai.github.io/langgraphjs/how-tos/multi-agent-multi-turn-convo/)

See the [multi-agent tutorials](https://langchain-ai.github.io/langgraphjs/tutorials/index/#multi-agent-systems) for implementations of other multi-agent architectures.

See the below guides for how-to implement multi-agent workflows with the [Functional API](https://langchain-ai.github.io/langgraphjs/concepts/functional_api/):

- [How to build a multi-agent network (functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/multi-agent-network-functional/)
- [How to add multi-turn conversation in a multi-agent application (functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/multi-agent-multi-turn-convo-functional/)

### State management

- [How to define graph state](https://langchain-ai.github.io/langgraphjs/how-tos/define-state/)
- [Have a separate input and output schema](https://langchain-ai.github.io/langgraphjs/how-tos/input_output_schema/)
- [Pass private state between nodes inside the graph](https://langchain-ai.github.io/langgraphjs/how-tos/pass_private_state/)

### Other

- [How to add runtime configuration to your graph](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)
- [How to add node retries](https://langchain-ai.github.io/langgraphjs/how-tos/node-retry-policies/)
- [How to cache expensive nodes](https://langchain-ai.github.io/langgraphjs/how-tos/node-caching/)
- [How to let an agent return tool results directly](https://langchain-ai.github.io/langgraphjs/how-tos/dynamically-returning-directly/)
- [How to have an agent respond in structured format](https://langchain-ai.github.io/langgraphjs/how-tos/respond-in-format/)
- [How to manage agent steps](https://langchain-ai.github.io/langgraphjs/how-tos/managing-agent-steps/)

### Prebuilt ReAct Agent

- [How to create a ReAct agent](https://langchain-ai.github.io/langgraphjs/how-tos/create-react-agent/)
- [How to add memory to a ReAct agent](https://langchain-ai.github.io/langgraphjs/how-tos/react-memory/)
- [How to add a system prompt to a ReAct agent](https://langchain-ai.github.io/langgraphjs/how-tos/react-system-prompt/)
- [How to add Human-in-the-loop to a ReAct agent](https://langchain-ai.github.io/langgraphjs/how-tos/react-human-in-the-loop/)
- [How to return structured output from a ReAct agent](https://langchain-ai.github.io/langgraphjs/how-tos/react-return-structured-output/)

See the below guide for how-to build ReAct agents with the [Functional API](https://langchain-ai.github.io/langgraphjs/concepts/functional_api/):

- [How to create a ReAct agent from scratch (Functional API)](https://langchain-ai.github.io/langgraphjs/how-tos/react-agent-from-scratch-functional/)

## LangGraph Platform

This section includes how-to guides for LangGraph Platform.

LangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework. It provides four deployment options to fit a range of needs: a free tier, a self-hosted version, a cloud SaaS, and a Bring Your Own Cloud (BYOC) option. You can explore these options in detail in the [deployment options guide](https://langchain-ai.github.io/langgraphjs/concepts/deployment_options/).

!!! tip

    * LangGraph is an MIT-licensed open-source library, which we are committed to maintaining and growing for the community.
    * You can always deploy LangGraph applications on your own infrastructure using the open-source LangGraph project without using LangGraph Platform.

### Application Structure

Learn how to set up your app for deployment to LangGraph Platform:

- [How to set up app for deployment (requirements.txt)](https://langchain-ai.github.io/langgraphjs/cloud/deployment/setup/)
- [How to set up app for deployment (pyproject.toml)](https://langchain-ai.github.io/langgraphjs/cloud/deployment/setup_pyproject/)
- [How to set up app for deployment (JavaScript)](https://langchain-ai.github.io/langgraphjs/cloud/deployment/setup_javascript/)
- [How to customize Dockerfile](https://langchain-ai.github.io/langgraphjs/cloud/deployment/custom_docker/)
- [How to test locally](https://langchain-ai.github.io/langgraphjs/cloud/deployment/test_locally/)
- [How to integrate LangGraph into your React application](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/use_stream_react/)

### Deployment

LangGraph applications can be deployed using LangGraph Cloud, which provides a range of services to help you deploy, manage, and scale your applications.

- [How to deploy to LangGraph cloud](https://langchain-ai.github.io/langgraphjs/cloud/deployment/cloud/)
- [How to deploy to a self-hosted environment](https://langchain-ai.github.io/langgraphjs/how-tos/deploy-self-hosted/)
- [How to interact with the deployment using RemoteGraph](https://langchain-ai.github.io/langgraphjs/how-tos/use-remote-graph/)

### Assistants

[Assistants](https://langchain-ai.github.io/langgraphjs/concepts/assistants/) are a configured instance of a template.

- [How to configure agents](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/configuration_cloud/)
- [How to version assistants](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/assistant_versioning/)

### Threads

- [How to copy threads](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/copy_threads/)
- [How to check status of your threads](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/check_thread_status/)

### Runs

LangGraph Cloud supports multiple types of runs besides streaming runs.

- [How to run an agent in the background](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/background_run/)
- [How to run multiple agents in the same thread](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/same-thread/)
- [How to create cron jobs](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/cron_jobs/)
- [How to create stateless runs](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stateless_runs/)

### Streaming

Streaming the results of your LLM application is vital for ensuring a good user experience, especially when your graph may call multiple models and take a long time to fully complete a run. Read about how to stream values from your graph in these how to guides:

- [How to stream values](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_values/)
- [How to stream updates](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_updates/)
- [How to stream messages](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_messages/)
- [How to stream events](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_events/)
- [How to stream in debug mode](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_debug/)
- [How to stream multiple modes](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/stream_multiple/)

### Frontend & Generative UI

With LangGraph Platform you can integrate LangGraph agents into your React applications and colocate UI components with your agent code.

- [How to integrate LangGraph into your React application](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/use_stream_react/)
- [How to implement Generative User Interfaces with LangGraph](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/generative_ui_react/)

### Human-in-the-loop

When creating complex graphs, leaving every decision up to the LLM can be dangerous, especially when the decisions involve invoking certain tools or accessing specific documents. To remedy this, LangGraph allows you to insert human-in-the-loop behavior to ensure your graph does not have undesired outcomes. Read more about the different ways you can add human-in-the-loop capabilities to your LangGraph Cloud projects in these how-to guides:

- [How to add a breakpoint](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/human_in_the_loop_breakpoint/)
- [How to wait for user input](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/human_in_the_loop_user_input/)
- [How to edit graph state](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/human_in_the_loop_edit_state/)
- [How to replay and branch from prior states](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/human_in_the_loop_time_travel/)
- [How to review tool calls](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/human_in_the_loop_review_tool_calls/)

### Double-texting

Graph execution can take a while, and sometimes users may change their mind about the input they wanted to send before their original input has finished running. For example, a user might notice a typo in their original request and will edit the prompt and resend it. Deciding what to do in these cases is important for ensuring a smooth user experience and preventing your graphs from behaving in unexpected ways. The following how-to guides provide information on the various options LangGraph Cloud gives you for dealing with double-texting:

- [How to use the interrupt option](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/interrupt_concurrent/)
- [How to use the rollback option](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/rollback_concurrent/)
- [How to use the reject option](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/reject_concurrent/)
- [How to use the enqueue option](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/enqueue_concurrent/)

### Webhooks

- [How to integrate webhooks](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/webhooks/)

### Cron Jobs

- [How to create cron jobs](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/cron_jobs/)

### LangGraph Studio

LangGraph Studio is a built-in UI for visualizing, testing, and debugging your agents.

- [How to connect to a LangGraph Cloud deployment](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/test_deployment/)
- [How to connect to a local deployment](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/test_local_deployment/)
- [How to test your graph in LangGraph Studio](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/invoke_studio/)
- [How to interact with threads in LangGraph Studio](https://langchain-ai.github.io/langgraphjs/cloud/how-tos/threads_studio/)

## Troubleshooting

These are the guides for resolving common errors you may find while building with LangGraph. Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

- [GRAPH_RECURSION_LIMIT](https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/GRAPH_RECURSION_LIMIT/)
- [INVALID_CONCURRENT_GRAPH_UPDATE](https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE/)
- [INVALID_GRAPH_NODE_RETURN_VALUE](https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE/)
- [MULTIPLE_SUBGRAPHS](https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/MULTIPLE_SUBGRAPHS/)
- [UNREACHABLE_NODE](https://langchain-ai.github.io/langgraphjs/troubleshooting/errors/UNREACHABLE_NODE/)